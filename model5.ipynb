{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWVUnPZ2JYz8SdMV1m1TeS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e839f872875544f3a1587f5957a7c17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a78b9d54f9b4a38a8c3f469377e55e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5690a7f67db2493b8a89a9edf22d0bae",
              "IPY_MODEL_06abc747ad2f410cad8bc7e5b0e20d23"
            ]
          }
        },
        "1a78b9d54f9b4a38a8c3f469377e55e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5690a7f67db2493b8a89a9edf22d0bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fac94802eb7140dcad55b879cf2770d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_821d49a6dcf548b1bbbdce709291ba51"
          }
        },
        "06abc747ad2f410cad8bc7e5b0e20d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d21637d69fc4a7e8606aaa84520a648",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [10:19&lt;00:00, 15995.37it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f11b2f9fe18446a886628add938ac28"
          }
        },
        "fac94802eb7140dcad55b879cf2770d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "821d49a6dcf548b1bbbdce709291ba51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d21637d69fc4a7e8606aaa84520a648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f11b2f9fe18446a886628add938ac28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e91e613768504ed69dde764920f4226c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8116283a0d8c414d8f60480693b279ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2aa38c5f497240afb04773c7f41f257c",
              "IPY_MODEL_2c5c850c1a7645fd847a300ac21d8cc3"
            ]
          }
        },
        "8116283a0d8c414d8f60480693b279ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2aa38c5f497240afb04773c7f41f257c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b2f8aa232044689a9056d2cc1eb9430",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3ce72fc470f4bf89a31ca9d5a57615c"
          }
        },
        "2c5c850c1a7645fd847a300ac21d8cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bedd229519eb45799959f96fdd806572",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [05:33&lt;00:00, 88.97it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4dbc72e99f445afa49ef75abfb6f52b"
          }
        },
        "5b2f8aa232044689a9056d2cc1eb9430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3ce72fc470f4bf89a31ca9d5a57615c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bedd229519eb45799959f96fdd806572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4dbc72e99f445afa49ef75abfb6f52b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a98405876084616ae28a166f4a0ac38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5cf97623b80b4d64b967b61985e75dc0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b798b269f3e0491886c3f5fd33e54439",
              "IPY_MODEL_7dba6daac8534185907462af63fbfff1"
            ]
          }
        },
        "5cf97623b80b4d64b967b61985e75dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b798b269f3e0491886c3f5fd33e54439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eb5d6ea841384994988c1d9b1503799e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08912b2c921e49c0986a6e7be9a7f56a"
          }
        },
        "7dba6daac8534185907462af63fbfff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0733cc29d2094944b4d26e984230c610",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:49&lt;00:00, 33220.26it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ba53f8aa7074e5eb3cc99d53a5f9b80"
          }
        },
        "eb5d6ea841384994988c1d9b1503799e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08912b2c921e49c0986a6e7be9a7f56a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0733cc29d2094944b4d26e984230c610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ba53f8aa7074e5eb3cc99d53a5f9b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b0876d926564aaa94fb11c2a6fe948f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a144ec2f12c462695e2ad14e0294a1b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_84c93655b5d641b08f10aaf311f58229",
              "IPY_MODEL_6523c9288b0d479b9f80df565102be32"
            ]
          }
        },
        "5a144ec2f12c462695e2ad14e0294a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84c93655b5d641b08f10aaf311f58229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dd1fa9f7ff034ff7a18033f763f1a57d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7bb5909926a54337aaf72d6c075d1765"
          }
        },
        "6523c9288b0d479b9f80df565102be32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f44a5ab39e024eebb743f20304fed49f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:00&lt;00:00, 18857.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_428973a7f1964e209c22da55c9ea9ad8"
          }
        },
        "dd1fa9f7ff034ff7a18033f763f1a57d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7bb5909926a54337aaf72d6c075d1765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f44a5ab39e024eebb743f20304fed49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "428973a7f1964e209c22da55c9ea9ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tumblingdice512/Research/blob/master/model5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e839f872875544f3a1587f5957a7c17d",
            "1a78b9d54f9b4a38a8c3f469377e55e1",
            "5690a7f67db2493b8a89a9edf22d0bae",
            "06abc747ad2f410cad8bc7e5b0e20d23",
            "fac94802eb7140dcad55b879cf2770d7",
            "821d49a6dcf548b1bbbdce709291ba51",
            "4d21637d69fc4a7e8606aaa84520a648",
            "8f11b2f9fe18446a886628add938ac28",
            "e91e613768504ed69dde764920f4226c",
            "8116283a0d8c414d8f60480693b279ca",
            "2aa38c5f497240afb04773c7f41f257c",
            "2c5c850c1a7645fd847a300ac21d8cc3",
            "5b2f8aa232044689a9056d2cc1eb9430",
            "e3ce72fc470f4bf89a31ca9d5a57615c",
            "bedd229519eb45799959f96fdd806572",
            "a4dbc72e99f445afa49ef75abfb6f52b",
            "7a98405876084616ae28a166f4a0ac38",
            "5cf97623b80b4d64b967b61985e75dc0",
            "b798b269f3e0491886c3f5fd33e54439",
            "7dba6daac8534185907462af63fbfff1",
            "eb5d6ea841384994988c1d9b1503799e",
            "08912b2c921e49c0986a6e7be9a7f56a",
            "0733cc29d2094944b4d26e984230c610",
            "0ba53f8aa7074e5eb3cc99d53a5f9b80",
            "6b0876d926564aaa94fb11c2a6fe948f",
            "5a144ec2f12c462695e2ad14e0294a1b",
            "84c93655b5d641b08f10aaf311f58229",
            "6523c9288b0d479b9f80df565102be32",
            "dd1fa9f7ff034ff7a18033f763f1a57d",
            "7bb5909926a54337aaf72d6c075d1765",
            "f44a5ab39e024eebb743f20304fed49f",
            "428973a7f1964e209c22da55c9ea9ad8"
          ]
        },
        "id": "G5-3J-5chJLe",
        "outputId": "8a425ff5-90e8-40a1-c644-43c007e750c9"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as f\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        " \n",
        "class Network1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 1000, False)\n",
        "        self.fc2 = torch.nn.Linear(1000, 10, False)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        " \n",
        "        return f.log_softmax(x, dim=1)\n",
        " \n",
        " \n",
        "def load_MNIST(batch=128, intensity=1.0):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('./data',\n",
        "                       train=True,\n",
        "                       download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Lambda(lambda x: x * intensity)\n",
        "                       ])),\n",
        "        batch_size=batch,\n",
        "        shuffle=True)\n",
        " \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('./data',\n",
        "                       train=False,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Lambda(lambda x: x * intensity)\n",
        "                       ])),\n",
        "        batch_size=batch,\n",
        "        shuffle=True)\n",
        " \n",
        "    return {'train': train_loader, 'test': test_loader}\n",
        " \n",
        " \n",
        "if __name__ == '__main__':\n",
        "    # 学習回数\n",
        "    epoch = 20\n",
        " \n",
        "    # 学習結果の保存用\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "    }\n",
        " \n",
        "    # ネットワークを構築\n",
        "    net: torch.nn.Module = Network1()\n",
        " \n",
        "    # MNISTのデータローダーを取得\n",
        "    loaders = load_MNIST()\n",
        " \n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        " \n",
        "    for e in range(epoch):\n",
        " \n",
        "        \"\"\" Training Part\"\"\"\n",
        "        loss = None\n",
        "        # 学習開始 (再開)\n",
        "        net.train(True)  # 引数は省略可能\n",
        "        for i, (data, target) in enumerate(loaders['train']):\n",
        "            # 全結合のみのネットワークでは入力を1次元に\n",
        "            # print(data.shape)  # torch.Size([128, 1, 28, 28])\n",
        "            data = data.view(-1, 28*28)\n",
        "            # print(data.shape)  # torch.Size([128, 784])\n",
        " \n",
        "            optimizer.zero_grad()\n",
        "            output = net(data)\n",
        "            loss = f.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        " \n",
        "            if i % 10 == 0:\n",
        "                print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(e+1,\n",
        "                                                                                         (i+1)*128,\n",
        "                                                                                         loss.item())\n",
        "                      )\n",
        " \n",
        "        history['train_loss'].append(loss)\n",
        " \n",
        "        \"\"\" Test Part \"\"\"\n",
        "        # 学習のストップ\n",
        "        net.eval()  # または net.train(False) でも良い\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        " \n",
        "        with torch.no_grad():\n",
        "            for data, target in loaders['test']:\n",
        "                data = data.view(-1, 28 * 28)\n",
        "                output = net(data)\n",
        "                test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        " \n",
        "        test_loss /= 10000\n",
        " \n",
        "        print('Test loss (avg): {}, Accuracy: {}'.format(test_loss,\n",
        "                                                         correct / 10000))\n",
        " \n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(correct / 10000)\n",
        " \n",
        "    # 結果の出力と描画\n",
        "    print(history)\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epoch+1), history['train_loss'], label='train_loss')\n",
        "    plt.plot(range(1, epoch+1), history['test_loss'], label='test_loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend()\n",
        "    plt.savefig('loss.png')\n",
        " \n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epoch+1), history['test_acc'])\n",
        "    plt.title('test accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.savefig('test_acc.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e839f872875544f3a1587f5957a7c17d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e91e613768504ed69dde764920f4226c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a98405876084616ae28a166f4a0ac38",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b0876d926564aaa94fb11c2a6fe948f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.435809373855591\n",
            "Training log: 1 epoch (1408 / 60000 train. data). Loss: 1.8232277631759644\n",
            "Training log: 1 epoch (2688 / 60000 train. data). Loss: 1.375369668006897\n",
            "Training log: 1 epoch (3968 / 60000 train. data). Loss: 1.2031720876693726\n",
            "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.8374272584915161\n",
            "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.7198622822761536\n",
            "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.6596168875694275\n",
            "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.5850553512573242\n",
            "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.4894704818725586\n",
            "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.5056400895118713\n",
            "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.44230204820632935\n",
            "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.44393062591552734\n",
            "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.48126766085624695\n",
            "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.4284447431564331\n",
            "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.30638256669044495\n",
            "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.4480144679546356\n",
            "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.4008385241031647\n",
            "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.3303486108779907\n",
            "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.4453423321247101\n",
            "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.38739725947380066\n",
            "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.35959991812705994\n",
            "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.38942819833755493\n",
            "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.30388954281806946\n",
            "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.3162580728530884\n",
            "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.35759079456329346\n",
            "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.36709195375442505\n",
            "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.2894311845302582\n",
            "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.29903826117515564\n",
            "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.4536445736885071\n",
            "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.32106533646583557\n",
            "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.2529761493206024\n",
            "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.27179303765296936\n",
            "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.2847382128238678\n",
            "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.28719907999038696\n",
            "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.24764209985733032\n",
            "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.3817037045955658\n",
            "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.4436156749725342\n",
            "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.31377512216567993\n",
            "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.30379363894462585\n",
            "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.25673341751098633\n",
            "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.13868066668510437\n",
            "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.3127620816230774\n",
            "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.3822956085205078\n",
            "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.289685994386673\n",
            "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.2960782051086426\n",
            "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.38775938749313354\n",
            "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.5181403160095215\n",
            "Test loss (avg): 0.25674401404857633, Accuracy: 0.927\n",
            "Training log: 2 epoch (128 / 60000 train. data). Loss: 0.28372666239738464\n",
            "Training log: 2 epoch (1408 / 60000 train. data). Loss: 0.16950882971286774\n",
            "Training log: 2 epoch (2688 / 60000 train. data). Loss: 0.3586732745170593\n",
            "Training log: 2 epoch (3968 / 60000 train. data). Loss: 0.32317155599594116\n",
            "Training log: 2 epoch (5248 / 60000 train. data). Loss: 0.21870088577270508\n",
            "Training log: 2 epoch (6528 / 60000 train. data). Loss: 0.3102456033229828\n",
            "Training log: 2 epoch (7808 / 60000 train. data). Loss: 0.29271870851516724\n",
            "Training log: 2 epoch (9088 / 60000 train. data). Loss: 0.22672583162784576\n",
            "Training log: 2 epoch (10368 / 60000 train. data). Loss: 0.19693900644779205\n",
            "Training log: 2 epoch (11648 / 60000 train. data). Loss: 0.3342272937297821\n",
            "Training log: 2 epoch (12928 / 60000 train. data). Loss: 0.2433582991361618\n",
            "Training log: 2 epoch (14208 / 60000 train. data). Loss: 0.4090307652950287\n",
            "Training log: 2 epoch (15488 / 60000 train. data). Loss: 0.23419229686260223\n",
            "Training log: 2 epoch (16768 / 60000 train. data). Loss: 0.2574126124382019\n",
            "Training log: 2 epoch (18048 / 60000 train. data). Loss: 0.36240726709365845\n",
            "Training log: 2 epoch (19328 / 60000 train. data). Loss: 0.1272595375776291\n",
            "Training log: 2 epoch (20608 / 60000 train. data). Loss: 0.18046414852142334\n",
            "Training log: 2 epoch (21888 / 60000 train. data). Loss: 0.21757525205612183\n",
            "Training log: 2 epoch (23168 / 60000 train. data). Loss: 0.26937633752822876\n",
            "Training log: 2 epoch (24448 / 60000 train. data). Loss: 0.32680898904800415\n",
            "Training log: 2 epoch (25728 / 60000 train. data). Loss: 0.1485249102115631\n",
            "Training log: 2 epoch (27008 / 60000 train. data). Loss: 0.2800670862197876\n",
            "Training log: 2 epoch (28288 / 60000 train. data). Loss: 0.14133992791175842\n",
            "Training log: 2 epoch (29568 / 60000 train. data). Loss: 0.3349793553352356\n",
            "Training log: 2 epoch (30848 / 60000 train. data). Loss: 0.38106077909469604\n",
            "Training log: 2 epoch (32128 / 60000 train. data). Loss: 0.3446892499923706\n",
            "Training log: 2 epoch (33408 / 60000 train. data). Loss: 0.22046837210655212\n",
            "Training log: 2 epoch (34688 / 60000 train. data). Loss: 0.24521978199481964\n",
            "Training log: 2 epoch (35968 / 60000 train. data). Loss: 0.10036927461624146\n",
            "Training log: 2 epoch (37248 / 60000 train. data). Loss: 0.29486551880836487\n",
            "Training log: 2 epoch (38528 / 60000 train. data). Loss: 0.17723862826824188\n",
            "Training log: 2 epoch (39808 / 60000 train. data). Loss: 0.09305962175130844\n",
            "Training log: 2 epoch (41088 / 60000 train. data). Loss: 0.2074299156665802\n",
            "Training log: 2 epoch (42368 / 60000 train. data). Loss: 0.19966599345207214\n",
            "Training log: 2 epoch (43648 / 60000 train. data). Loss: 0.18707048892974854\n",
            "Training log: 2 epoch (44928 / 60000 train. data). Loss: 0.19896042346954346\n",
            "Training log: 2 epoch (46208 / 60000 train. data). Loss: 0.16593247652053833\n",
            "Training log: 2 epoch (47488 / 60000 train. data). Loss: 0.15955224633216858\n",
            "Training log: 2 epoch (48768 / 60000 train. data). Loss: 0.1824219822883606\n",
            "Training log: 2 epoch (50048 / 60000 train. data). Loss: 0.20786860585212708\n",
            "Training log: 2 epoch (51328 / 60000 train. data). Loss: 0.19017137587070465\n",
            "Training log: 2 epoch (52608 / 60000 train. data). Loss: 0.2357524037361145\n",
            "Training log: 2 epoch (53888 / 60000 train. data). Loss: 0.2466934770345688\n",
            "Training log: 2 epoch (55168 / 60000 train. data). Loss: 0.16975685954093933\n",
            "Training log: 2 epoch (56448 / 60000 train. data). Loss: 0.16140218079090118\n",
            "Training log: 2 epoch (57728 / 60000 train. data). Loss: 0.264573872089386\n",
            "Training log: 2 epoch (59008 / 60000 train. data). Loss: 0.2806035578250885\n",
            "Test loss (avg): 0.1992448383331299, Accuracy: 0.9415\n",
            "Training log: 3 epoch (128 / 60000 train. data). Loss: 0.2272387593984604\n",
            "Training log: 3 epoch (1408 / 60000 train. data). Loss: 0.17032542824745178\n",
            "Training log: 3 epoch (2688 / 60000 train. data). Loss: 0.16743190586566925\n",
            "Training log: 3 epoch (3968 / 60000 train. data). Loss: 0.3006620407104492\n",
            "Training log: 3 epoch (5248 / 60000 train. data). Loss: 0.19750943779945374\n",
            "Training log: 3 epoch (6528 / 60000 train. data). Loss: 0.1952199637889862\n",
            "Training log: 3 epoch (7808 / 60000 train. data). Loss: 0.22551079094409943\n",
            "Training log: 3 epoch (9088 / 60000 train. data). Loss: 0.10394340008497238\n",
            "Training log: 3 epoch (10368 / 60000 train. data). Loss: 0.17026497423648834\n",
            "Training log: 3 epoch (11648 / 60000 train. data). Loss: 0.13858790695667267\n",
            "Training log: 3 epoch (12928 / 60000 train. data). Loss: 0.2240860015153885\n",
            "Training log: 3 epoch (14208 / 60000 train. data). Loss: 0.13343289494514465\n",
            "Training log: 3 epoch (15488 / 60000 train. data). Loss: 0.1575533002614975\n",
            "Training log: 3 epoch (16768 / 60000 train. data). Loss: 0.21015678346157074\n",
            "Training log: 3 epoch (18048 / 60000 train. data). Loss: 0.2316904217004776\n",
            "Training log: 3 epoch (19328 / 60000 train. data). Loss: 0.20778056979179382\n",
            "Training log: 3 epoch (20608 / 60000 train. data). Loss: 0.20584337413311005\n",
            "Training log: 3 epoch (21888 / 60000 train. data). Loss: 0.16335667669773102\n",
            "Training log: 3 epoch (23168 / 60000 train. data). Loss: 0.15100263059139252\n",
            "Training log: 3 epoch (24448 / 60000 train. data). Loss: 0.20570451021194458\n",
            "Training log: 3 epoch (25728 / 60000 train. data). Loss: 0.16648168861865997\n",
            "Training log: 3 epoch (27008 / 60000 train. data). Loss: 0.19922244548797607\n",
            "Training log: 3 epoch (28288 / 60000 train. data). Loss: 0.1692308485507965\n",
            "Training log: 3 epoch (29568 / 60000 train. data). Loss: 0.13747073709964752\n",
            "Training log: 3 epoch (30848 / 60000 train. data). Loss: 0.15423932671546936\n",
            "Training log: 3 epoch (32128 / 60000 train. data). Loss: 0.1771584451198578\n",
            "Training log: 3 epoch (33408 / 60000 train. data). Loss: 0.28084859251976013\n",
            "Training log: 3 epoch (34688 / 60000 train. data). Loss: 0.13021492958068848\n",
            "Training log: 3 epoch (35968 / 60000 train. data). Loss: 0.19712182879447937\n",
            "Training log: 3 epoch (37248 / 60000 train. data). Loss: 0.19286848604679108\n",
            "Training log: 3 epoch (38528 / 60000 train. data). Loss: 0.19588133692741394\n",
            "Training log: 3 epoch (39808 / 60000 train. data). Loss: 0.1621786504983902\n",
            "Training log: 3 epoch (41088 / 60000 train. data). Loss: 0.15342599153518677\n",
            "Training log: 3 epoch (42368 / 60000 train. data). Loss: 0.18738213181495667\n",
            "Training log: 3 epoch (43648 / 60000 train. data). Loss: 0.17407485842704773\n",
            "Training log: 3 epoch (44928 / 60000 train. data). Loss: 0.3043281137943268\n",
            "Training log: 3 epoch (46208 / 60000 train. data). Loss: 0.2530986964702606\n",
            "Training log: 3 epoch (47488 / 60000 train. data). Loss: 0.24313493072986603\n",
            "Training log: 3 epoch (48768 / 60000 train. data). Loss: 0.17006152868270874\n",
            "Training log: 3 epoch (50048 / 60000 train. data). Loss: 0.19814646244049072\n",
            "Training log: 3 epoch (51328 / 60000 train. data). Loss: 0.13458925485610962\n",
            "Training log: 3 epoch (52608 / 60000 train. data). Loss: 0.19611813127994537\n",
            "Training log: 3 epoch (53888 / 60000 train. data). Loss: 0.10983607172966003\n",
            "Training log: 3 epoch (55168 / 60000 train. data). Loss: 0.12590691447257996\n",
            "Training log: 3 epoch (56448 / 60000 train. data). Loss: 0.10118169337511063\n",
            "Training log: 3 epoch (57728 / 60000 train. data). Loss: 0.13515311479568481\n",
            "Training log: 3 epoch (59008 / 60000 train. data). Loss: 0.12946783006191254\n",
            "Test loss (avg): 0.15682375156879425, Accuracy: 0.9547\n",
            "Training log: 4 epoch (128 / 60000 train. data). Loss: 0.1950637251138687\n",
            "Training log: 4 epoch (1408 / 60000 train. data). Loss: 0.2823389768600464\n",
            "Training log: 4 epoch (2688 / 60000 train. data). Loss: 0.135253444314003\n",
            "Training log: 4 epoch (3968 / 60000 train. data). Loss: 0.09051672369241714\n",
            "Training log: 4 epoch (5248 / 60000 train. data). Loss: 0.17412780225276947\n",
            "Training log: 4 epoch (6528 / 60000 train. data). Loss: 0.09913376718759537\n",
            "Training log: 4 epoch (7808 / 60000 train. data). Loss: 0.2661556899547577\n",
            "Training log: 4 epoch (9088 / 60000 train. data). Loss: 0.09634707123041153\n",
            "Training log: 4 epoch (10368 / 60000 train. data). Loss: 0.19932855665683746\n",
            "Training log: 4 epoch (11648 / 60000 train. data). Loss: 0.09424600750207901\n",
            "Training log: 4 epoch (12928 / 60000 train. data). Loss: 0.12690454721450806\n",
            "Training log: 4 epoch (14208 / 60000 train. data). Loss: 0.13624201714992523\n",
            "Training log: 4 epoch (15488 / 60000 train. data). Loss: 0.16309121251106262\n",
            "Training log: 4 epoch (16768 / 60000 train. data). Loss: 0.0996856763958931\n",
            "Training log: 4 epoch (18048 / 60000 train. data). Loss: 0.12624384462833405\n",
            "Training log: 4 epoch (19328 / 60000 train. data). Loss: 0.19389623403549194\n",
            "Training log: 4 epoch (20608 / 60000 train. data). Loss: 0.06808696687221527\n",
            "Training log: 4 epoch (21888 / 60000 train. data). Loss: 0.12455624341964722\n",
            "Training log: 4 epoch (23168 / 60000 train. data). Loss: 0.1669265478849411\n",
            "Training log: 4 epoch (24448 / 60000 train. data). Loss: 0.12055806070566177\n",
            "Training log: 4 epoch (25728 / 60000 train. data). Loss: 0.07624056935310364\n",
            "Training log: 4 epoch (27008 / 60000 train. data). Loss: 0.05754195526242256\n",
            "Training log: 4 epoch (28288 / 60000 train. data). Loss: 0.09440246224403381\n",
            "Training log: 4 epoch (29568 / 60000 train. data). Loss: 0.14944905042648315\n",
            "Training log: 4 epoch (30848 / 60000 train. data). Loss: 0.1746429055929184\n",
            "Training log: 4 epoch (32128 / 60000 train. data). Loss: 0.15863755345344543\n",
            "Training log: 4 epoch (33408 / 60000 train. data). Loss: 0.09518735110759735\n",
            "Training log: 4 epoch (34688 / 60000 train. data). Loss: 0.11550460755825043\n",
            "Training log: 4 epoch (35968 / 60000 train. data). Loss: 0.15624111890792847\n",
            "Training log: 4 epoch (37248 / 60000 train. data). Loss: 0.244985431432724\n",
            "Training log: 4 epoch (38528 / 60000 train. data). Loss: 0.08914005011320114\n",
            "Training log: 4 epoch (39808 / 60000 train. data). Loss: 0.11033111810684204\n",
            "Training log: 4 epoch (41088 / 60000 train. data). Loss: 0.13738349080085754\n",
            "Training log: 4 epoch (42368 / 60000 train. data). Loss: 0.14911559224128723\n",
            "Training log: 4 epoch (43648 / 60000 train. data). Loss: 0.12109782546758652\n",
            "Training log: 4 epoch (44928 / 60000 train. data). Loss: 0.09653844684362411\n",
            "Training log: 4 epoch (46208 / 60000 train. data). Loss: 0.11136134713888168\n",
            "Training log: 4 epoch (47488 / 60000 train. data). Loss: 0.0803019180893898\n",
            "Training log: 4 epoch (48768 / 60000 train. data). Loss: 0.13605782389640808\n",
            "Training log: 4 epoch (50048 / 60000 train. data). Loss: 0.13568361103534698\n",
            "Training log: 4 epoch (51328 / 60000 train. data). Loss: 0.11102332174777985\n",
            "Training log: 4 epoch (52608 / 60000 train. data). Loss: 0.15974478423595428\n",
            "Training log: 4 epoch (53888 / 60000 train. data). Loss: 0.20219837129116058\n",
            "Training log: 4 epoch (55168 / 60000 train. data). Loss: 0.0996546596288681\n",
            "Training log: 4 epoch (56448 / 60000 train. data). Loss: 0.15919280052185059\n",
            "Training log: 4 epoch (57728 / 60000 train. data). Loss: 0.10634129494428635\n",
            "Training log: 4 epoch (59008 / 60000 train. data). Loss: 0.16242964565753937\n",
            "Test loss (avg): 0.1302484982252121, Accuracy: 0.9599\n",
            "Training log: 5 epoch (128 / 60000 train. data). Loss: 0.08196452260017395\n",
            "Training log: 5 epoch (1408 / 60000 train. data). Loss: 0.12469641864299774\n",
            "Training log: 5 epoch (2688 / 60000 train. data). Loss: 0.15635599195957184\n",
            "Training log: 5 epoch (3968 / 60000 train. data). Loss: 0.07394622266292572\n",
            "Training log: 5 epoch (5248 / 60000 train. data). Loss: 0.09645575284957886\n",
            "Training log: 5 epoch (6528 / 60000 train. data). Loss: 0.11680667102336884\n",
            "Training log: 5 epoch (7808 / 60000 train. data). Loss: 0.08596090227365494\n",
            "Training log: 5 epoch (9088 / 60000 train. data). Loss: 0.17409004271030426\n",
            "Training log: 5 epoch (10368 / 60000 train. data). Loss: 0.04725424945354462\n",
            "Training log: 5 epoch (11648 / 60000 train. data). Loss: 0.11582555621862411\n",
            "Training log: 5 epoch (12928 / 60000 train. data). Loss: 0.11246147751808167\n",
            "Training log: 5 epoch (14208 / 60000 train. data). Loss: 0.18095794320106506\n",
            "Training log: 5 epoch (15488 / 60000 train. data). Loss: 0.059769101440906525\n",
            "Training log: 5 epoch (16768 / 60000 train. data). Loss: 0.07913019508123398\n",
            "Training log: 5 epoch (18048 / 60000 train. data). Loss: 0.10044272989034653\n",
            "Training log: 5 epoch (19328 / 60000 train. data). Loss: 0.09271784126758575\n",
            "Training log: 5 epoch (20608 / 60000 train. data). Loss: 0.10362891107797623\n",
            "Training log: 5 epoch (21888 / 60000 train. data). Loss: 0.13435840606689453\n",
            "Training log: 5 epoch (23168 / 60000 train. data). Loss: 0.09285304695367813\n",
            "Training log: 5 epoch (24448 / 60000 train. data). Loss: 0.18152178823947906\n",
            "Training log: 5 epoch (25728 / 60000 train. data). Loss: 0.193496435880661\n",
            "Training log: 5 epoch (27008 / 60000 train. data). Loss: 0.07759568095207214\n",
            "Training log: 5 epoch (28288 / 60000 train. data). Loss: 0.2129756063222885\n",
            "Training log: 5 epoch (29568 / 60000 train. data). Loss: 0.08877211809158325\n",
            "Training log: 5 epoch (30848 / 60000 train. data). Loss: 0.12534113228321075\n",
            "Training log: 5 epoch (32128 / 60000 train. data). Loss: 0.10458194464445114\n",
            "Training log: 5 epoch (33408 / 60000 train. data). Loss: 0.07979951053857803\n",
            "Training log: 5 epoch (34688 / 60000 train. data). Loss: 0.14058537781238556\n",
            "Training log: 5 epoch (35968 / 60000 train. data). Loss: 0.18301276862621307\n",
            "Training log: 5 epoch (37248 / 60000 train. data). Loss: 0.10022628307342529\n",
            "Training log: 5 epoch (38528 / 60000 train. data). Loss: 0.12382666766643524\n",
            "Training log: 5 epoch (39808 / 60000 train. data). Loss: 0.10993963479995728\n",
            "Training log: 5 epoch (41088 / 60000 train. data). Loss: 0.0867876186966896\n",
            "Training log: 5 epoch (42368 / 60000 train. data). Loss: 0.14073345065116882\n",
            "Training log: 5 epoch (43648 / 60000 train. data). Loss: 0.07104437798261642\n",
            "Training log: 5 epoch (44928 / 60000 train. data). Loss: 0.17896349728107452\n",
            "Training log: 5 epoch (46208 / 60000 train. data). Loss: 0.06635470688343048\n",
            "Training log: 5 epoch (47488 / 60000 train. data). Loss: 0.07999985665082932\n",
            "Training log: 5 epoch (48768 / 60000 train. data). Loss: 0.08828470855951309\n",
            "Training log: 5 epoch (50048 / 60000 train. data). Loss: 0.21669058501720428\n",
            "Training log: 5 epoch (51328 / 60000 train. data). Loss: 0.060626596212387085\n",
            "Training log: 5 epoch (52608 / 60000 train. data). Loss: 0.20551779866218567\n",
            "Training log: 5 epoch (53888 / 60000 train. data). Loss: 0.08642852306365967\n",
            "Training log: 5 epoch (55168 / 60000 train. data). Loss: 0.06766562163829803\n",
            "Training log: 5 epoch (56448 / 60000 train. data). Loss: 0.11481299996376038\n",
            "Training log: 5 epoch (57728 / 60000 train. data). Loss: 0.10390754044055939\n",
            "Training log: 5 epoch (59008 / 60000 train. data). Loss: 0.08657082915306091\n",
            "Test loss (avg): 0.10790070136785507, Accuracy: 0.9672\n",
            "Training log: 6 epoch (128 / 60000 train. data). Loss: 0.04324629157781601\n",
            "Training log: 6 epoch (1408 / 60000 train. data). Loss: 0.0949658751487732\n",
            "Training log: 6 epoch (2688 / 60000 train. data). Loss: 0.10282397270202637\n",
            "Training log: 6 epoch (3968 / 60000 train. data). Loss: 0.08754681050777435\n",
            "Training log: 6 epoch (5248 / 60000 train. data). Loss: 0.03721565008163452\n",
            "Training log: 6 epoch (6528 / 60000 train. data). Loss: 0.10049239546060562\n",
            "Training log: 6 epoch (7808 / 60000 train. data). Loss: 0.06973437964916229\n",
            "Training log: 6 epoch (9088 / 60000 train. data). Loss: 0.06258340179920197\n",
            "Training log: 6 epoch (10368 / 60000 train. data). Loss: 0.07392112165689468\n",
            "Training log: 6 epoch (11648 / 60000 train. data). Loss: 0.04922182485461235\n",
            "Training log: 6 epoch (12928 / 60000 train. data). Loss: 0.07170617580413818\n",
            "Training log: 6 epoch (14208 / 60000 train. data). Loss: 0.09829256683588028\n",
            "Training log: 6 epoch (15488 / 60000 train. data). Loss: 0.11654708534479141\n",
            "Training log: 6 epoch (16768 / 60000 train. data). Loss: 0.057642776519060135\n",
            "Training log: 6 epoch (18048 / 60000 train. data). Loss: 0.11777331680059433\n",
            "Training log: 6 epoch (19328 / 60000 train. data). Loss: 0.0712677538394928\n",
            "Training log: 6 epoch (20608 / 60000 train. data). Loss: 0.09228832274675369\n",
            "Training log: 6 epoch (21888 / 60000 train. data). Loss: 0.15184207260608673\n",
            "Training log: 6 epoch (23168 / 60000 train. data). Loss: 0.10183656215667725\n",
            "Training log: 6 epoch (24448 / 60000 train. data). Loss: 0.10819661617279053\n",
            "Training log: 6 epoch (25728 / 60000 train. data). Loss: 0.12519267201423645\n",
            "Training log: 6 epoch (27008 / 60000 train. data). Loss: 0.19121631979942322\n",
            "Training log: 6 epoch (28288 / 60000 train. data). Loss: 0.059565864503383636\n",
            "Training log: 6 epoch (29568 / 60000 train. data). Loss: 0.11197599023580551\n",
            "Training log: 6 epoch (30848 / 60000 train. data). Loss: 0.08814424276351929\n",
            "Training log: 6 epoch (32128 / 60000 train. data). Loss: 0.11469553411006927\n",
            "Training log: 6 epoch (33408 / 60000 train. data). Loss: 0.07180667668581009\n",
            "Training log: 6 epoch (34688 / 60000 train. data). Loss: 0.07826243340969086\n",
            "Training log: 6 epoch (35968 / 60000 train. data). Loss: 0.1731492578983307\n",
            "Training log: 6 epoch (37248 / 60000 train. data). Loss: 0.11546777188777924\n",
            "Training log: 6 epoch (38528 / 60000 train. data). Loss: 0.0612826831638813\n",
            "Training log: 6 epoch (39808 / 60000 train. data). Loss: 0.0669909343123436\n",
            "Training log: 6 epoch (41088 / 60000 train. data). Loss: 0.09082473814487457\n",
            "Training log: 6 epoch (42368 / 60000 train. data). Loss: 0.09723920375108719\n",
            "Training log: 6 epoch (43648 / 60000 train. data). Loss: 0.0854276642203331\n",
            "Training log: 6 epoch (44928 / 60000 train. data). Loss: 0.1280454397201538\n",
            "Training log: 6 epoch (46208 / 60000 train. data). Loss: 0.1177932620048523\n",
            "Training log: 6 epoch (47488 / 60000 train. data). Loss: 0.046893179416656494\n",
            "Training log: 6 epoch (48768 / 60000 train. data). Loss: 0.061424966901540756\n",
            "Training log: 6 epoch (50048 / 60000 train. data). Loss: 0.04478791728615761\n",
            "Training log: 6 epoch (51328 / 60000 train. data). Loss: 0.11352362483739853\n",
            "Training log: 6 epoch (52608 / 60000 train. data). Loss: 0.09346169978380203\n",
            "Training log: 6 epoch (53888 / 60000 train. data). Loss: 0.07331981509923935\n",
            "Training log: 6 epoch (55168 / 60000 train. data). Loss: 0.07627814263105392\n",
            "Training log: 6 epoch (56448 / 60000 train. data). Loss: 0.0714474618434906\n",
            "Training log: 6 epoch (57728 / 60000 train. data). Loss: 0.12677760422229767\n",
            "Training log: 6 epoch (59008 / 60000 train. data). Loss: 0.1046360582113266\n",
            "Test loss (avg): 0.09192169861644506, Accuracy: 0.9718\n",
            "Training log: 7 epoch (128 / 60000 train. data). Loss: 0.08746371418237686\n",
            "Training log: 7 epoch (1408 / 60000 train. data). Loss: 0.06536950170993805\n",
            "Training log: 7 epoch (2688 / 60000 train. data). Loss: 0.07167361676692963\n",
            "Training log: 7 epoch (3968 / 60000 train. data). Loss: 0.06663341075181961\n",
            "Training log: 7 epoch (5248 / 60000 train. data). Loss: 0.07287650555372238\n",
            "Training log: 7 epoch (6528 / 60000 train. data). Loss: 0.03347873315215111\n",
            "Training log: 7 epoch (7808 / 60000 train. data). Loss: 0.07414620369672775\n",
            "Training log: 7 epoch (9088 / 60000 train. data). Loss: 0.038941022008657455\n",
            "Training log: 7 epoch (10368 / 60000 train. data). Loss: 0.0814286470413208\n",
            "Training log: 7 epoch (11648 / 60000 train. data). Loss: 0.06606894731521606\n",
            "Training log: 7 epoch (12928 / 60000 train. data). Loss: 0.036425262689590454\n",
            "Training log: 7 epoch (14208 / 60000 train. data). Loss: 0.030344923958182335\n",
            "Training log: 7 epoch (15488 / 60000 train. data). Loss: 0.16784700751304626\n",
            "Training log: 7 epoch (16768 / 60000 train. data). Loss: 0.048434674739837646\n",
            "Training log: 7 epoch (18048 / 60000 train. data). Loss: 0.08168425410985947\n",
            "Training log: 7 epoch (19328 / 60000 train. data). Loss: 0.10605058073997498\n",
            "Training log: 7 epoch (20608 / 60000 train. data). Loss: 0.083373062312603\n",
            "Training log: 7 epoch (21888 / 60000 train. data). Loss: 0.044336508959531784\n",
            "Training log: 7 epoch (23168 / 60000 train. data). Loss: 0.15840789675712585\n",
            "Training log: 7 epoch (24448 / 60000 train. data). Loss: 0.12467785179615021\n",
            "Training log: 7 epoch (25728 / 60000 train. data). Loss: 0.043721962720155716\n",
            "Training log: 7 epoch (27008 / 60000 train. data). Loss: 0.13832969963550568\n",
            "Training log: 7 epoch (28288 / 60000 train. data). Loss: 0.08999495208263397\n",
            "Training log: 7 epoch (29568 / 60000 train. data). Loss: 0.05131363123655319\n",
            "Training log: 7 epoch (30848 / 60000 train. data). Loss: 0.09162085503339767\n",
            "Training log: 7 epoch (32128 / 60000 train. data). Loss: 0.11682266741991043\n",
            "Training log: 7 epoch (33408 / 60000 train. data). Loss: 0.03349413350224495\n",
            "Training log: 7 epoch (34688 / 60000 train. data). Loss: 0.04449129104614258\n",
            "Training log: 7 epoch (35968 / 60000 train. data). Loss: 0.057608455419540405\n",
            "Training log: 7 epoch (37248 / 60000 train. data). Loss: 0.06596971303224564\n",
            "Training log: 7 epoch (38528 / 60000 train. data). Loss: 0.07532331347465515\n",
            "Training log: 7 epoch (39808 / 60000 train. data). Loss: 0.10297476500272751\n",
            "Training log: 7 epoch (41088 / 60000 train. data). Loss: 0.046042926609516144\n",
            "Training log: 7 epoch (42368 / 60000 train. data). Loss: 0.039947740733623505\n",
            "Training log: 7 epoch (43648 / 60000 train. data). Loss: 0.06662525981664658\n",
            "Training log: 7 epoch (44928 / 60000 train. data). Loss: 0.07527992874383926\n",
            "Training log: 7 epoch (46208 / 60000 train. data). Loss: 0.07301109284162521\n",
            "Training log: 7 epoch (47488 / 60000 train. data). Loss: 0.02497269958257675\n",
            "Training log: 7 epoch (48768 / 60000 train. data). Loss: 0.13887344300746918\n",
            "Training log: 7 epoch (50048 / 60000 train. data). Loss: 0.0948420912027359\n",
            "Training log: 7 epoch (51328 / 60000 train. data). Loss: 0.046153996139764786\n",
            "Training log: 7 epoch (52608 / 60000 train. data). Loss: 0.10294602811336517\n",
            "Training log: 7 epoch (53888 / 60000 train. data). Loss: 0.0406256765127182\n",
            "Training log: 7 epoch (55168 / 60000 train. data). Loss: 0.09561967104673386\n",
            "Training log: 7 epoch (56448 / 60000 train. data). Loss: 0.08837873488664627\n",
            "Training log: 7 epoch (57728 / 60000 train. data). Loss: 0.06872716546058655\n",
            "Training log: 7 epoch (59008 / 60000 train. data). Loss: 0.09964753687381744\n",
            "Test loss (avg): 0.08839927258491516, Accuracy: 0.9728\n",
            "Training log: 8 epoch (128 / 60000 train. data). Loss: 0.030351776629686356\n",
            "Training log: 8 epoch (1408 / 60000 train. data). Loss: 0.059113990515470505\n",
            "Training log: 8 epoch (2688 / 60000 train. data). Loss: 0.0664931908249855\n",
            "Training log: 8 epoch (3968 / 60000 train. data). Loss: 0.11452644318342209\n",
            "Training log: 8 epoch (5248 / 60000 train. data). Loss: 0.059264156967401505\n",
            "Training log: 8 epoch (6528 / 60000 train. data). Loss: 0.0862913504242897\n",
            "Training log: 8 epoch (7808 / 60000 train. data). Loss: 0.038493480533361435\n",
            "Training log: 8 epoch (9088 / 60000 train. data). Loss: 0.0857519581913948\n",
            "Training log: 8 epoch (10368 / 60000 train. data). Loss: 0.07937195152044296\n",
            "Training log: 8 epoch (11648 / 60000 train. data). Loss: 0.056615427136421204\n",
            "Training log: 8 epoch (12928 / 60000 train. data). Loss: 0.14968684315681458\n",
            "Training log: 8 epoch (14208 / 60000 train. data). Loss: 0.07143239676952362\n",
            "Training log: 8 epoch (15488 / 60000 train. data). Loss: 0.060577213764190674\n",
            "Training log: 8 epoch (16768 / 60000 train. data). Loss: 0.03493799269199371\n",
            "Training log: 8 epoch (18048 / 60000 train. data). Loss: 0.07579480856657028\n",
            "Training log: 8 epoch (19328 / 60000 train. data). Loss: 0.046810172498226166\n",
            "Training log: 8 epoch (20608 / 60000 train. data). Loss: 0.12250237166881561\n",
            "Training log: 8 epoch (21888 / 60000 train. data). Loss: 0.04775238037109375\n",
            "Training log: 8 epoch (23168 / 60000 train. data). Loss: 0.06264019012451172\n",
            "Training log: 8 epoch (24448 / 60000 train. data). Loss: 0.04435534030199051\n",
            "Training log: 8 epoch (25728 / 60000 train. data). Loss: 0.11548512428998947\n",
            "Training log: 8 epoch (27008 / 60000 train. data). Loss: 0.04250047728419304\n",
            "Training log: 8 epoch (28288 / 60000 train. data). Loss: 0.07993169128894806\n",
            "Training log: 8 epoch (29568 / 60000 train. data). Loss: 0.042312584817409515\n",
            "Training log: 8 epoch (30848 / 60000 train. data). Loss: 0.05784683674573898\n",
            "Training log: 8 epoch (32128 / 60000 train. data). Loss: 0.0476832315325737\n",
            "Training log: 8 epoch (33408 / 60000 train. data). Loss: 0.04416815936565399\n",
            "Training log: 8 epoch (34688 / 60000 train. data). Loss: 0.0648941770195961\n",
            "Training log: 8 epoch (35968 / 60000 train. data). Loss: 0.03341519832611084\n",
            "Training log: 8 epoch (37248 / 60000 train. data). Loss: 0.0628688707947731\n",
            "Training log: 8 epoch (38528 / 60000 train. data). Loss: 0.08682195842266083\n",
            "Training log: 8 epoch (39808 / 60000 train. data). Loss: 0.02985193021595478\n",
            "Training log: 8 epoch (41088 / 60000 train. data). Loss: 0.023546673357486725\n",
            "Training log: 8 epoch (42368 / 60000 train. data). Loss: 0.013065238483250141\n",
            "Training log: 8 epoch (43648 / 60000 train. data). Loss: 0.05452033132314682\n",
            "Training log: 8 epoch (44928 / 60000 train. data). Loss: 0.0852777287364006\n",
            "Training log: 8 epoch (46208 / 60000 train. data). Loss: 0.024365626275539398\n",
            "Training log: 8 epoch (47488 / 60000 train. data). Loss: 0.04561613127589226\n",
            "Training log: 8 epoch (48768 / 60000 train. data). Loss: 0.10911918431520462\n",
            "Training log: 8 epoch (50048 / 60000 train. data). Loss: 0.0436127670109272\n",
            "Training log: 8 epoch (51328 / 60000 train. data). Loss: 0.032990533858537674\n",
            "Training log: 8 epoch (52608 / 60000 train. data). Loss: 0.05878044292330742\n",
            "Training log: 8 epoch (53888 / 60000 train. data). Loss: 0.08707450330257416\n",
            "Training log: 8 epoch (55168 / 60000 train. data). Loss: 0.050263162702322006\n",
            "Training log: 8 epoch (56448 / 60000 train. data). Loss: 0.059595365077257156\n",
            "Training log: 8 epoch (57728 / 60000 train. data). Loss: 0.044718898832798004\n",
            "Training log: 8 epoch (59008 / 60000 train. data). Loss: 0.1218242198228836\n",
            "Test loss (avg): 0.08092695128023625, Accuracy: 0.9757\n",
            "Training log: 9 epoch (128 / 60000 train. data). Loss: 0.028602516278624535\n",
            "Training log: 9 epoch (1408 / 60000 train. data). Loss: 0.025559324771165848\n",
            "Training log: 9 epoch (2688 / 60000 train. data). Loss: 0.05648704618215561\n",
            "Training log: 9 epoch (3968 / 60000 train. data). Loss: 0.05393322929739952\n",
            "Training log: 9 epoch (5248 / 60000 train. data). Loss: 0.06967277079820633\n",
            "Training log: 9 epoch (6528 / 60000 train. data). Loss: 0.07446461170911789\n",
            "Training log: 9 epoch (7808 / 60000 train. data). Loss: 0.023035330697894096\n",
            "Training log: 9 epoch (9088 / 60000 train. data). Loss: 0.03736557811498642\n",
            "Training log: 9 epoch (10368 / 60000 train. data). Loss: 0.07800497859716415\n",
            "Training log: 9 epoch (11648 / 60000 train. data). Loss: 0.02937663532793522\n",
            "Training log: 9 epoch (12928 / 60000 train. data). Loss: 0.03928331285715103\n",
            "Training log: 9 epoch (14208 / 60000 train. data). Loss: 0.016484852880239487\n",
            "Training log: 9 epoch (15488 / 60000 train. data). Loss: 0.09458017349243164\n",
            "Training log: 9 epoch (16768 / 60000 train. data). Loss: 0.0856296643614769\n",
            "Training log: 9 epoch (18048 / 60000 train. data). Loss: 0.0757378414273262\n",
            "Training log: 9 epoch (19328 / 60000 train. data). Loss: 0.0598163902759552\n",
            "Training log: 9 epoch (20608 / 60000 train. data). Loss: 0.052078451961278915\n",
            "Training log: 9 epoch (21888 / 60000 train. data). Loss: 0.022014254704117775\n",
            "Training log: 9 epoch (23168 / 60000 train. data). Loss: 0.037943653762340546\n",
            "Training log: 9 epoch (24448 / 60000 train. data). Loss: 0.019872386008501053\n",
            "Training log: 9 epoch (25728 / 60000 train. data). Loss: 0.06459742039442062\n",
            "Training log: 9 epoch (27008 / 60000 train. data). Loss: 0.0323927104473114\n",
            "Training log: 9 epoch (28288 / 60000 train. data). Loss: 0.07707739621400833\n",
            "Training log: 9 epoch (29568 / 60000 train. data). Loss: 0.0519266352057457\n",
            "Training log: 9 epoch (30848 / 60000 train. data). Loss: 0.015407945960760117\n",
            "Training log: 9 epoch (32128 / 60000 train. data). Loss: 0.033109843730926514\n",
            "Training log: 9 epoch (33408 / 60000 train. data). Loss: 0.11381469666957855\n",
            "Training log: 9 epoch (34688 / 60000 train. data). Loss: 0.03323500230908394\n",
            "Training log: 9 epoch (35968 / 60000 train. data). Loss: 0.06877634674310684\n",
            "Training log: 9 epoch (37248 / 60000 train. data). Loss: 0.06783449649810791\n",
            "Training log: 9 epoch (38528 / 60000 train. data). Loss: 0.043841876089572906\n",
            "Training log: 9 epoch (39808 / 60000 train. data). Loss: 0.04301965609192848\n",
            "Training log: 9 epoch (41088 / 60000 train. data). Loss: 0.03959919139742851\n",
            "Training log: 9 epoch (42368 / 60000 train. data). Loss: 0.02317172661423683\n",
            "Training log: 9 epoch (43648 / 60000 train. data). Loss: 0.042912743985652924\n",
            "Training log: 9 epoch (44928 / 60000 train. data). Loss: 0.0332380011677742\n",
            "Training log: 9 epoch (46208 / 60000 train. data). Loss: 0.03808412700891495\n",
            "Training log: 9 epoch (47488 / 60000 train. data). Loss: 0.02928375080227852\n",
            "Training log: 9 epoch (48768 / 60000 train. data). Loss: 0.02559264563024044\n",
            "Training log: 9 epoch (50048 / 60000 train. data). Loss: 0.047261931002140045\n",
            "Training log: 9 epoch (51328 / 60000 train. data). Loss: 0.012152834795415401\n",
            "Training log: 9 epoch (52608 / 60000 train. data). Loss: 0.1044873297214508\n",
            "Training log: 9 epoch (53888 / 60000 train. data). Loss: 0.04244697839021683\n",
            "Training log: 9 epoch (55168 / 60000 train. data). Loss: 0.08607622981071472\n",
            "Training log: 9 epoch (56448 / 60000 train. data). Loss: 0.046539150178432465\n",
            "Training log: 9 epoch (57728 / 60000 train. data). Loss: 0.03620368614792824\n",
            "Training log: 9 epoch (59008 / 60000 train. data). Loss: 0.02000349760055542\n",
            "Test loss (avg): 0.07370994516387581, Accuracy: 0.9777\n",
            "Training log: 10 epoch (128 / 60000 train. data). Loss: 0.043448515236377716\n",
            "Training log: 10 epoch (1408 / 60000 train. data). Loss: 0.05026382580399513\n",
            "Training log: 10 epoch (2688 / 60000 train. data). Loss: 0.011273397132754326\n",
            "Training log: 10 epoch (3968 / 60000 train. data). Loss: 0.012326251715421677\n",
            "Training log: 10 epoch (5248 / 60000 train. data). Loss: 0.0350407175719738\n",
            "Training log: 10 epoch (6528 / 60000 train. data). Loss: 0.039722103625535965\n",
            "Training log: 10 epoch (7808 / 60000 train. data). Loss: 0.019864430651068687\n",
            "Training log: 10 epoch (9088 / 60000 train. data). Loss: 0.06950031220912933\n",
            "Training log: 10 epoch (10368 / 60000 train. data). Loss: 0.03465993329882622\n",
            "Training log: 10 epoch (11648 / 60000 train. data). Loss: 0.05125800520181656\n",
            "Training log: 10 epoch (12928 / 60000 train. data). Loss: 0.030125122517347336\n",
            "Training log: 10 epoch (14208 / 60000 train. data). Loss: 0.0399530865252018\n",
            "Training log: 10 epoch (15488 / 60000 train. data). Loss: 0.08482617884874344\n",
            "Training log: 10 epoch (16768 / 60000 train. data). Loss: 0.0255226232111454\n",
            "Training log: 10 epoch (18048 / 60000 train. data). Loss: 0.014918023720383644\n",
            "Training log: 10 epoch (19328 / 60000 train. data). Loss: 0.03258077800273895\n",
            "Training log: 10 epoch (20608 / 60000 train. data). Loss: 0.04276391118764877\n",
            "Training log: 10 epoch (21888 / 60000 train. data). Loss: 0.044086914509534836\n",
            "Training log: 10 epoch (23168 / 60000 train. data). Loss: 0.05884265899658203\n",
            "Training log: 10 epoch (24448 / 60000 train. data). Loss: 0.061537545174360275\n",
            "Training log: 10 epoch (25728 / 60000 train. data). Loss: 0.020570989698171616\n",
            "Training log: 10 epoch (27008 / 60000 train. data). Loss: 0.023016005754470825\n",
            "Training log: 10 epoch (28288 / 60000 train. data). Loss: 0.03049550950527191\n",
            "Training log: 10 epoch (29568 / 60000 train. data). Loss: 0.012259433977305889\n",
            "Training log: 10 epoch (30848 / 60000 train. data). Loss: 0.04125835746526718\n",
            "Training log: 10 epoch (32128 / 60000 train. data). Loss: 0.033360354602336884\n",
            "Training log: 10 epoch (33408 / 60000 train. data). Loss: 0.016421305015683174\n",
            "Training log: 10 epoch (34688 / 60000 train. data). Loss: 0.024749111384153366\n",
            "Training log: 10 epoch (35968 / 60000 train. data). Loss: 0.06884808838367462\n",
            "Training log: 10 epoch (37248 / 60000 train. data). Loss: 0.060026492923498154\n",
            "Training log: 10 epoch (38528 / 60000 train. data). Loss: 0.06688624620437622\n",
            "Training log: 10 epoch (39808 / 60000 train. data). Loss: 0.07398024201393127\n",
            "Training log: 10 epoch (41088 / 60000 train. data). Loss: 0.007617231458425522\n",
            "Training log: 10 epoch (42368 / 60000 train. data). Loss: 0.012881672941148281\n",
            "Training log: 10 epoch (43648 / 60000 train. data). Loss: 0.03212208300828934\n",
            "Training log: 10 epoch (44928 / 60000 train. data). Loss: 0.03442930057644844\n",
            "Training log: 10 epoch (46208 / 60000 train. data). Loss: 0.05240394547581673\n",
            "Training log: 10 epoch (47488 / 60000 train. data). Loss: 0.01108131930232048\n",
            "Training log: 10 epoch (48768 / 60000 train. data). Loss: 0.053091514855623245\n",
            "Training log: 10 epoch (50048 / 60000 train. data). Loss: 0.021927714347839355\n",
            "Training log: 10 epoch (51328 / 60000 train. data). Loss: 0.028362926095724106\n",
            "Training log: 10 epoch (52608 / 60000 train. data). Loss: 0.022592125460505486\n",
            "Training log: 10 epoch (53888 / 60000 train. data). Loss: 0.026387207210063934\n",
            "Training log: 10 epoch (55168 / 60000 train. data). Loss: 0.06160268187522888\n",
            "Training log: 10 epoch (56448 / 60000 train. data). Loss: 0.03173651546239853\n",
            "Training log: 10 epoch (57728 / 60000 train. data). Loss: 0.013076958246529102\n",
            "Training log: 10 epoch (59008 / 60000 train. data). Loss: 0.06695133447647095\n",
            "Test loss (avg): 0.07024191123247146, Accuracy: 0.9798\n",
            "Training log: 11 epoch (128 / 60000 train. data). Loss: 0.020593011751770973\n",
            "Training log: 11 epoch (1408 / 60000 train. data). Loss: 0.05672218278050423\n",
            "Training log: 11 epoch (2688 / 60000 train. data). Loss: 0.051687393337488174\n",
            "Training log: 11 epoch (3968 / 60000 train. data). Loss: 0.011430243961513042\n",
            "Training log: 11 epoch (5248 / 60000 train. data). Loss: 0.04615115746855736\n",
            "Training log: 11 epoch (6528 / 60000 train. data). Loss: 0.026045409962534904\n",
            "Training log: 11 epoch (7808 / 60000 train. data). Loss: 0.028981849551200867\n",
            "Training log: 11 epoch (9088 / 60000 train. data). Loss: 0.03159749135375023\n",
            "Training log: 11 epoch (10368 / 60000 train. data). Loss: 0.012596160173416138\n",
            "Training log: 11 epoch (11648 / 60000 train. data). Loss: 0.05573071911931038\n",
            "Training log: 11 epoch (12928 / 60000 train. data). Loss: 0.03028287924826145\n",
            "Training log: 11 epoch (14208 / 60000 train. data). Loss: 0.05547669157385826\n",
            "Training log: 11 epoch (15488 / 60000 train. data). Loss: 0.02156863361597061\n",
            "Training log: 11 epoch (16768 / 60000 train. data). Loss: 0.055200617760419846\n",
            "Training log: 11 epoch (18048 / 60000 train. data). Loss: 0.02359512634575367\n",
            "Training log: 11 epoch (19328 / 60000 train. data). Loss: 0.1071249470114708\n",
            "Training log: 11 epoch (20608 / 60000 train. data). Loss: 0.04103231430053711\n",
            "Training log: 11 epoch (21888 / 60000 train. data). Loss: 0.01714302785694599\n",
            "Training log: 11 epoch (23168 / 60000 train. data). Loss: 0.010206025093793869\n",
            "Training log: 11 epoch (24448 / 60000 train. data). Loss: 0.01723649352788925\n",
            "Training log: 11 epoch (25728 / 60000 train. data). Loss: 0.031647488474845886\n",
            "Training log: 11 epoch (27008 / 60000 train. data). Loss: 0.041689980775117874\n",
            "Training log: 11 epoch (28288 / 60000 train. data). Loss: 0.028853457421064377\n",
            "Training log: 11 epoch (29568 / 60000 train. data). Loss: 0.02948879450559616\n",
            "Training log: 11 epoch (30848 / 60000 train. data). Loss: 0.016284480690956116\n",
            "Training log: 11 epoch (32128 / 60000 train. data). Loss: 0.021506771445274353\n",
            "Training log: 11 epoch (33408 / 60000 train. data). Loss: 0.015764351934194565\n",
            "Training log: 11 epoch (34688 / 60000 train. data). Loss: 0.019417859613895416\n",
            "Training log: 11 epoch (35968 / 60000 train. data). Loss: 0.02012627013027668\n",
            "Training log: 11 epoch (37248 / 60000 train. data). Loss: 0.022231288254261017\n",
            "Training log: 11 epoch (38528 / 60000 train. data). Loss: 0.025266990065574646\n",
            "Training log: 11 epoch (39808 / 60000 train. data). Loss: 0.02298871800303459\n",
            "Training log: 11 epoch (41088 / 60000 train. data). Loss: 0.052167460322380066\n",
            "Training log: 11 epoch (42368 / 60000 train. data). Loss: 0.015837496146559715\n",
            "Training log: 11 epoch (43648 / 60000 train. data). Loss: 0.019370028749108315\n",
            "Training log: 11 epoch (44928 / 60000 train. data). Loss: 0.041042182594537735\n",
            "Training log: 11 epoch (46208 / 60000 train. data). Loss: 0.021693002432584763\n",
            "Training log: 11 epoch (47488 / 60000 train. data). Loss: 0.019740255549550056\n",
            "Training log: 11 epoch (48768 / 60000 train. data). Loss: 0.04029028117656708\n",
            "Training log: 11 epoch (50048 / 60000 train. data). Loss: 0.027939630672335625\n",
            "Training log: 11 epoch (51328 / 60000 train. data). Loss: 0.016094909980893135\n",
            "Training log: 11 epoch (52608 / 60000 train. data). Loss: 0.08477696031332016\n",
            "Training log: 11 epoch (53888 / 60000 train. data). Loss: 0.04315180331468582\n",
            "Training log: 11 epoch (55168 / 60000 train. data). Loss: 0.01561175100505352\n",
            "Training log: 11 epoch (56448 / 60000 train. data). Loss: 0.012234322726726532\n",
            "Training log: 11 epoch (57728 / 60000 train. data). Loss: 0.0356697179377079\n",
            "Training log: 11 epoch (59008 / 60000 train. data). Loss: 0.05591615289449692\n",
            "Test loss (avg): 0.0662525423616171, Accuracy: 0.9802\n",
            "Training log: 12 epoch (128 / 60000 train. data). Loss: 0.07306176424026489\n",
            "Training log: 12 epoch (1408 / 60000 train. data). Loss: 0.010494407266378403\n",
            "Training log: 12 epoch (2688 / 60000 train. data). Loss: 0.038793809711933136\n",
            "Training log: 12 epoch (3968 / 60000 train. data). Loss: 0.01842285320162773\n",
            "Training log: 12 epoch (5248 / 60000 train. data). Loss: 0.01805025525391102\n",
            "Training log: 12 epoch (6528 / 60000 train. data). Loss: 0.018838107585906982\n",
            "Training log: 12 epoch (7808 / 60000 train. data). Loss: 0.023788828402757645\n",
            "Training log: 12 epoch (9088 / 60000 train. data). Loss: 0.01244835089892149\n",
            "Training log: 12 epoch (10368 / 60000 train. data). Loss: 0.018278570845723152\n",
            "Training log: 12 epoch (11648 / 60000 train. data). Loss: 0.023443417623639107\n",
            "Training log: 12 epoch (12928 / 60000 train. data). Loss: 0.035142652690410614\n",
            "Training log: 12 epoch (14208 / 60000 train. data). Loss: 0.030285606160759926\n",
            "Training log: 12 epoch (15488 / 60000 train. data). Loss: 0.042299337685108185\n",
            "Training log: 12 epoch (16768 / 60000 train. data). Loss: 0.024065254256129265\n",
            "Training log: 12 epoch (18048 / 60000 train. data). Loss: 0.0696183443069458\n",
            "Training log: 12 epoch (19328 / 60000 train. data). Loss: 0.03703439235687256\n",
            "Training log: 12 epoch (20608 / 60000 train. data). Loss: 0.046651944518089294\n",
            "Training log: 12 epoch (21888 / 60000 train. data). Loss: 0.02948220632970333\n",
            "Training log: 12 epoch (23168 / 60000 train. data). Loss: 0.02975635975599289\n",
            "Training log: 12 epoch (24448 / 60000 train. data). Loss: 0.049909498542547226\n",
            "Training log: 12 epoch (25728 / 60000 train. data). Loss: 0.029422873631119728\n",
            "Training log: 12 epoch (27008 / 60000 train. data). Loss: 0.025196710601449013\n",
            "Training log: 12 epoch (28288 / 60000 train. data). Loss: 0.021607844159007072\n",
            "Training log: 12 epoch (29568 / 60000 train. data). Loss: 0.006401991471648216\n",
            "Training log: 12 epoch (30848 / 60000 train. data). Loss: 0.041667740792036057\n",
            "Training log: 12 epoch (32128 / 60000 train. data). Loss: 0.0749039426445961\n",
            "Training log: 12 epoch (33408 / 60000 train. data). Loss: 0.010200176388025284\n",
            "Training log: 12 epoch (34688 / 60000 train. data). Loss: 0.014436325058341026\n",
            "Training log: 12 epoch (35968 / 60000 train. data). Loss: 0.015828171744942665\n",
            "Training log: 12 epoch (37248 / 60000 train. data). Loss: 0.01619122549891472\n",
            "Training log: 12 epoch (38528 / 60000 train. data). Loss: 0.045398712158203125\n",
            "Training log: 12 epoch (39808 / 60000 train. data). Loss: 0.016935931518673897\n",
            "Training log: 12 epoch (41088 / 60000 train. data). Loss: 0.008705191314220428\n",
            "Training log: 12 epoch (42368 / 60000 train. data). Loss: 0.00927478913217783\n",
            "Training log: 12 epoch (43648 / 60000 train. data). Loss: 0.011495298705995083\n",
            "Training log: 12 epoch (44928 / 60000 train. data). Loss: 0.03761176019906998\n",
            "Training log: 12 epoch (46208 / 60000 train. data). Loss: 0.028062252327799797\n",
            "Training log: 12 epoch (47488 / 60000 train. data). Loss: 0.019333316013216972\n",
            "Training log: 12 epoch (48768 / 60000 train. data). Loss: 0.010333149693906307\n",
            "Training log: 12 epoch (50048 / 60000 train. data). Loss: 0.015232402831315994\n",
            "Training log: 12 epoch (51328 / 60000 train. data). Loss: 0.04198557883501053\n",
            "Training log: 12 epoch (52608 / 60000 train. data). Loss: 0.021922335028648376\n",
            "Training log: 12 epoch (53888 / 60000 train. data). Loss: 0.04664850980043411\n",
            "Training log: 12 epoch (55168 / 60000 train. data). Loss: 0.01903912052512169\n",
            "Training log: 12 epoch (56448 / 60000 train. data). Loss: 0.03668804094195366\n",
            "Training log: 12 epoch (57728 / 60000 train. data). Loss: 0.01376380492001772\n",
            "Training log: 12 epoch (59008 / 60000 train. data). Loss: 0.0052056703716516495\n",
            "Test loss (avg): 0.06529531195606104, Accuracy: 0.9797\n",
            "Training log: 13 epoch (128 / 60000 train. data). Loss: 0.00865145493298769\n",
            "Training log: 13 epoch (1408 / 60000 train. data). Loss: 0.030116064473986626\n",
            "Training log: 13 epoch (2688 / 60000 train. data). Loss: 0.013393097557127476\n",
            "Training log: 13 epoch (3968 / 60000 train. data). Loss: 0.011544031091034412\n",
            "Training log: 13 epoch (5248 / 60000 train. data). Loss: 0.010687594301998615\n",
            "Training log: 13 epoch (6528 / 60000 train. data). Loss: 0.0062219807878136635\n",
            "Training log: 13 epoch (7808 / 60000 train. data). Loss: 0.03363979235291481\n",
            "Training log: 13 epoch (9088 / 60000 train. data). Loss: 0.006253957282751799\n",
            "Training log: 13 epoch (10368 / 60000 train. data). Loss: 0.010124349035322666\n",
            "Training log: 13 epoch (11648 / 60000 train. data). Loss: 0.02797994576394558\n",
            "Training log: 13 epoch (12928 / 60000 train. data). Loss: 0.020996956154704094\n",
            "Training log: 13 epoch (14208 / 60000 train. data). Loss: 0.02471371553838253\n",
            "Training log: 13 epoch (15488 / 60000 train. data). Loss: 0.021015124395489693\n",
            "Training log: 13 epoch (16768 / 60000 train. data). Loss: 0.013813197612762451\n",
            "Training log: 13 epoch (18048 / 60000 train. data). Loss: 0.014099529013037682\n",
            "Training log: 13 epoch (19328 / 60000 train. data). Loss: 0.017142679542303085\n",
            "Training log: 13 epoch (20608 / 60000 train. data). Loss: 0.025545617565512657\n",
            "Training log: 13 epoch (21888 / 60000 train. data). Loss: 0.01673160120844841\n",
            "Training log: 13 epoch (23168 / 60000 train. data). Loss: 0.011485000140964985\n",
            "Training log: 13 epoch (24448 / 60000 train. data). Loss: 0.008040182292461395\n",
            "Training log: 13 epoch (25728 / 60000 train. data). Loss: 0.009172264486551285\n",
            "Training log: 13 epoch (27008 / 60000 train. data). Loss: 0.004055428318679333\n",
            "Training log: 13 epoch (28288 / 60000 train. data). Loss: 0.029990149661898613\n",
            "Training log: 13 epoch (29568 / 60000 train. data). Loss: 0.022243089973926544\n",
            "Training log: 13 epoch (30848 / 60000 train. data). Loss: 0.019351782277226448\n",
            "Training log: 13 epoch (32128 / 60000 train. data). Loss: 0.017147807404398918\n",
            "Training log: 13 epoch (33408 / 60000 train. data). Loss: 0.008857384324073792\n",
            "Training log: 13 epoch (34688 / 60000 train. data). Loss: 0.026102963835000992\n",
            "Training log: 13 epoch (35968 / 60000 train. data). Loss: 0.03466358408331871\n",
            "Training log: 13 epoch (37248 / 60000 train. data). Loss: 0.01065045315772295\n",
            "Training log: 13 epoch (38528 / 60000 train. data). Loss: 0.008269204758107662\n",
            "Training log: 13 epoch (39808 / 60000 train. data). Loss: 0.019574912264943123\n",
            "Training log: 13 epoch (41088 / 60000 train. data). Loss: 0.020857132971286774\n",
            "Training log: 13 epoch (42368 / 60000 train. data). Loss: 0.016507448628544807\n",
            "Training log: 13 epoch (43648 / 60000 train. data). Loss: 0.043779850006103516\n",
            "Training log: 13 epoch (44928 / 60000 train. data). Loss: 0.022472813725471497\n",
            "Training log: 13 epoch (46208 / 60000 train. data). Loss: 0.025323688983917236\n",
            "Training log: 13 epoch (47488 / 60000 train. data). Loss: 0.01398819126188755\n",
            "Training log: 13 epoch (48768 / 60000 train. data). Loss: 0.0275835283100605\n",
            "Training log: 13 epoch (50048 / 60000 train. data). Loss: 0.0257464200258255\n",
            "Training log: 13 epoch (51328 / 60000 train. data). Loss: 0.02487814798951149\n",
            "Training log: 13 epoch (52608 / 60000 train. data). Loss: 0.0711580291390419\n",
            "Training log: 13 epoch (53888 / 60000 train. data). Loss: 0.01720501109957695\n",
            "Training log: 13 epoch (55168 / 60000 train. data). Loss: 0.017096208408474922\n",
            "Training log: 13 epoch (56448 / 60000 train. data). Loss: 0.03287539258599281\n",
            "Training log: 13 epoch (57728 / 60000 train. data). Loss: 0.012078912928700447\n",
            "Training log: 13 epoch (59008 / 60000 train. data). Loss: 0.023002231493592262\n",
            "Test loss (avg): 0.06840249712541699, Accuracy: 0.9807\n",
            "Training log: 14 epoch (128 / 60000 train. data). Loss: 0.029903855174779892\n",
            "Training log: 14 epoch (1408 / 60000 train. data). Loss: 0.021051878109574318\n",
            "Training log: 14 epoch (2688 / 60000 train. data). Loss: 0.018241416662931442\n",
            "Training log: 14 epoch (3968 / 60000 train. data). Loss: 0.016049277037382126\n",
            "Training log: 14 epoch (5248 / 60000 train. data). Loss: 0.006731715518981218\n",
            "Training log: 14 epoch (6528 / 60000 train. data). Loss: 0.018144924193620682\n",
            "Training log: 14 epoch (7808 / 60000 train. data). Loss: 0.010601183399558067\n",
            "Training log: 14 epoch (9088 / 60000 train. data). Loss: 0.014610648155212402\n",
            "Training log: 14 epoch (10368 / 60000 train. data). Loss: 0.013606149703264236\n",
            "Training log: 14 epoch (11648 / 60000 train. data). Loss: 0.008256159722805023\n",
            "Training log: 14 epoch (12928 / 60000 train. data). Loss: 0.0235332939773798\n",
            "Training log: 14 epoch (14208 / 60000 train. data). Loss: 0.003686669748276472\n",
            "Training log: 14 epoch (15488 / 60000 train. data). Loss: 0.007841117680072784\n",
            "Training log: 14 epoch (16768 / 60000 train. data). Loss: 0.02045031450688839\n",
            "Training log: 14 epoch (18048 / 60000 train. data). Loss: 0.027678633108735085\n",
            "Training log: 14 epoch (19328 / 60000 train. data). Loss: 0.04004821553826332\n",
            "Training log: 14 epoch (20608 / 60000 train. data). Loss: 0.003541784593835473\n",
            "Training log: 14 epoch (21888 / 60000 train. data). Loss: 0.013639029115438461\n",
            "Training log: 14 epoch (23168 / 60000 train. data). Loss: 0.002202698029577732\n",
            "Training log: 14 epoch (24448 / 60000 train. data). Loss: 0.0166903268545866\n",
            "Training log: 14 epoch (25728 / 60000 train. data). Loss: 0.0048815528862178326\n",
            "Training log: 14 epoch (27008 / 60000 train. data). Loss: 0.021950336173176765\n",
            "Training log: 14 epoch (28288 / 60000 train. data). Loss: 0.008447492495179176\n",
            "Training log: 14 epoch (29568 / 60000 train. data). Loss: 0.021974962204694748\n",
            "Training log: 14 epoch (30848 / 60000 train. data). Loss: 0.015030224807560444\n",
            "Training log: 14 epoch (32128 / 60000 train. data). Loss: 0.01351187564432621\n",
            "Training log: 14 epoch (33408 / 60000 train. data). Loss: 0.012877007946372032\n",
            "Training log: 14 epoch (34688 / 60000 train. data). Loss: 0.01859361305832863\n",
            "Training log: 14 epoch (35968 / 60000 train. data). Loss: 0.008165496401488781\n",
            "Training log: 14 epoch (37248 / 60000 train. data). Loss: 0.0065787858329713345\n",
            "Training log: 14 epoch (38528 / 60000 train. data). Loss: 0.0064425235614180565\n",
            "Training log: 14 epoch (39808 / 60000 train. data). Loss: 0.03599878028035164\n",
            "Training log: 14 epoch (41088 / 60000 train. data). Loss: 0.013353013433516026\n",
            "Training log: 14 epoch (42368 / 60000 train. data). Loss: 0.03886060789227486\n",
            "Training log: 14 epoch (43648 / 60000 train. data). Loss: 0.01355284359306097\n",
            "Training log: 14 epoch (44928 / 60000 train. data). Loss: 0.0066889929585158825\n",
            "Training log: 14 epoch (46208 / 60000 train. data). Loss: 0.006245301105082035\n",
            "Training log: 14 epoch (47488 / 60000 train. data). Loss: 0.07512320578098297\n",
            "Training log: 14 epoch (48768 / 60000 train. data). Loss: 0.013139665126800537\n",
            "Training log: 14 epoch (50048 / 60000 train. data). Loss: 0.005451112985610962\n",
            "Training log: 14 epoch (51328 / 60000 train. data). Loss: 0.006426945794373751\n",
            "Training log: 14 epoch (52608 / 60000 train. data). Loss: 0.008688554167747498\n",
            "Training log: 14 epoch (53888 / 60000 train. data). Loss: 0.016984546557068825\n",
            "Training log: 14 epoch (55168 / 60000 train. data). Loss: 0.011825197376310825\n",
            "Training log: 14 epoch (56448 / 60000 train. data). Loss: 0.006874748505651951\n",
            "Training log: 14 epoch (57728 / 60000 train. data). Loss: 0.07200175523757935\n",
            "Training log: 14 epoch (59008 / 60000 train. data). Loss: 0.0521322637796402\n",
            "Test loss (avg): 0.0712637786924839, Accuracy: 0.9791\n",
            "Training log: 15 epoch (128 / 60000 train. data). Loss: 0.02600502409040928\n",
            "Training log: 15 epoch (1408 / 60000 train. data). Loss: 0.03781687468290329\n",
            "Training log: 15 epoch (2688 / 60000 train. data). Loss: 0.00778899434953928\n",
            "Training log: 15 epoch (3968 / 60000 train. data). Loss: 0.016183748841285706\n",
            "Training log: 15 epoch (5248 / 60000 train. data). Loss: 0.007412790786474943\n",
            "Training log: 15 epoch (6528 / 60000 train. data). Loss: 0.003680126741528511\n",
            "Training log: 15 epoch (7808 / 60000 train. data). Loss: 0.015781842172145844\n",
            "Training log: 15 epoch (9088 / 60000 train. data). Loss: 0.004658250603824854\n",
            "Training log: 15 epoch (10368 / 60000 train. data). Loss: 0.030484221875667572\n",
            "Training log: 15 epoch (11648 / 60000 train. data). Loss: 0.010141567327082157\n",
            "Training log: 15 epoch (12928 / 60000 train. data). Loss: 0.010011712089180946\n",
            "Training log: 15 epoch (14208 / 60000 train. data). Loss: 0.037535909563302994\n",
            "Training log: 15 epoch (15488 / 60000 train. data). Loss: 0.009614617563784122\n",
            "Training log: 15 epoch (16768 / 60000 train. data). Loss: 0.009558911435306072\n",
            "Training log: 15 epoch (18048 / 60000 train. data). Loss: 0.01831723004579544\n",
            "Training log: 15 epoch (19328 / 60000 train. data). Loss: 0.00765228783711791\n",
            "Training log: 15 epoch (20608 / 60000 train. data). Loss: 0.03170442581176758\n",
            "Training log: 15 epoch (21888 / 60000 train. data). Loss: 0.01807185262441635\n",
            "Training log: 15 epoch (23168 / 60000 train. data). Loss: 0.0040899780578911304\n",
            "Training log: 15 epoch (24448 / 60000 train. data). Loss: 0.019367815926671028\n",
            "Training log: 15 epoch (25728 / 60000 train. data). Loss: 0.008805976249277592\n",
            "Training log: 15 epoch (27008 / 60000 train. data). Loss: 0.02275066450238228\n",
            "Training log: 15 epoch (28288 / 60000 train. data). Loss: 0.004072148352861404\n",
            "Training log: 15 epoch (29568 / 60000 train. data). Loss: 0.016362804919481277\n",
            "Training log: 15 epoch (30848 / 60000 train. data). Loss: 0.0070733181200921535\n",
            "Training log: 15 epoch (32128 / 60000 train. data). Loss: 0.01013495959341526\n",
            "Training log: 15 epoch (33408 / 60000 train. data). Loss: 0.010512011125683784\n",
            "Training log: 15 epoch (34688 / 60000 train. data). Loss: 0.02244766242802143\n",
            "Training log: 15 epoch (35968 / 60000 train. data). Loss: 0.005059402901679277\n",
            "Training log: 15 epoch (37248 / 60000 train. data). Loss: 0.0038560284301638603\n",
            "Training log: 15 epoch (38528 / 60000 train. data). Loss: 0.0034375221002846956\n",
            "Training log: 15 epoch (39808 / 60000 train. data). Loss: 0.006329072639346123\n",
            "Training log: 15 epoch (41088 / 60000 train. data). Loss: 0.016975749284029007\n",
            "Training log: 15 epoch (42368 / 60000 train. data). Loss: 0.01026887632906437\n",
            "Training log: 15 epoch (43648 / 60000 train. data). Loss: 0.021809455007314682\n",
            "Training log: 15 epoch (44928 / 60000 train. data). Loss: 0.008953845128417015\n",
            "Training log: 15 epoch (46208 / 60000 train. data). Loss: 0.008253992535173893\n",
            "Training log: 15 epoch (47488 / 60000 train. data). Loss: 0.01090479176491499\n",
            "Training log: 15 epoch (48768 / 60000 train. data). Loss: 0.0077019729651510715\n",
            "Training log: 15 epoch (50048 / 60000 train. data). Loss: 0.016842428594827652\n",
            "Training log: 15 epoch (51328 / 60000 train. data). Loss: 0.010675624944269657\n",
            "Training log: 15 epoch (52608 / 60000 train. data). Loss: 0.009610163047909737\n",
            "Training log: 15 epoch (53888 / 60000 train. data). Loss: 0.017361504957079887\n",
            "Training log: 15 epoch (55168 / 60000 train. data). Loss: 0.013356559909880161\n",
            "Training log: 15 epoch (56448 / 60000 train. data). Loss: 0.012632130645215511\n",
            "Training log: 15 epoch (57728 / 60000 train. data). Loss: 0.012924515642225742\n",
            "Training log: 15 epoch (59008 / 60000 train. data). Loss: 0.004250081721693277\n",
            "Test loss (avg): 0.06652733830176294, Accuracy: 0.9806\n",
            "Training log: 16 epoch (128 / 60000 train. data). Loss: 0.01181591022759676\n",
            "Training log: 16 epoch (1408 / 60000 train. data). Loss: 0.009651191532611847\n",
            "Training log: 16 epoch (2688 / 60000 train. data). Loss: 0.01235191896557808\n",
            "Training log: 16 epoch (3968 / 60000 train. data). Loss: 0.020630227401852608\n",
            "Training log: 16 epoch (5248 / 60000 train. data). Loss: 0.018442124128341675\n",
            "Training log: 16 epoch (6528 / 60000 train. data). Loss: 0.015073638409376144\n",
            "Training log: 16 epoch (7808 / 60000 train. data). Loss: 0.0049034832045435905\n",
            "Training log: 16 epoch (9088 / 60000 train. data). Loss: 0.0043459124863147736\n",
            "Training log: 16 epoch (10368 / 60000 train. data). Loss: 0.0063013057224452496\n",
            "Training log: 16 epoch (11648 / 60000 train. data). Loss: 0.006250161677598953\n",
            "Training log: 16 epoch (12928 / 60000 train. data). Loss: 0.0047968532890081406\n",
            "Training log: 16 epoch (14208 / 60000 train. data). Loss: 0.005641063675284386\n",
            "Training log: 16 epoch (15488 / 60000 train. data). Loss: 0.00855827983468771\n",
            "Training log: 16 epoch (16768 / 60000 train. data). Loss: 0.012808026745915413\n",
            "Training log: 16 epoch (18048 / 60000 train. data). Loss: 0.009254805743694305\n",
            "Training log: 16 epoch (19328 / 60000 train. data). Loss: 0.013086124323308468\n",
            "Training log: 16 epoch (20608 / 60000 train. data). Loss: 0.016587743535637856\n",
            "Training log: 16 epoch (21888 / 60000 train. data). Loss: 0.006690823007375002\n",
            "Training log: 16 epoch (23168 / 60000 train. data). Loss: 0.011291337199509144\n",
            "Training log: 16 epoch (24448 / 60000 train. data). Loss: 0.011980433948338032\n",
            "Training log: 16 epoch (25728 / 60000 train. data). Loss: 0.006055847276002169\n",
            "Training log: 16 epoch (27008 / 60000 train. data). Loss: 0.007908041588962078\n",
            "Training log: 16 epoch (28288 / 60000 train. data). Loss: 0.015001730062067509\n",
            "Training log: 16 epoch (29568 / 60000 train. data). Loss: 0.03432435914874077\n",
            "Training log: 16 epoch (30848 / 60000 train. data). Loss: 0.0290412325412035\n",
            "Training log: 16 epoch (32128 / 60000 train. data). Loss: 0.008916380815207958\n",
            "Training log: 16 epoch (33408 / 60000 train. data). Loss: 0.009338006377220154\n",
            "Training log: 16 epoch (34688 / 60000 train. data). Loss: 0.007775930222123861\n",
            "Training log: 16 epoch (35968 / 60000 train. data). Loss: 0.030401673167943954\n",
            "Training log: 16 epoch (37248 / 60000 train. data). Loss: 0.006909229326993227\n",
            "Training log: 16 epoch (38528 / 60000 train. data). Loss: 0.002910957671701908\n",
            "Training log: 16 epoch (39808 / 60000 train. data). Loss: 0.017894966527819633\n",
            "Training log: 16 epoch (41088 / 60000 train. data). Loss: 0.007745415437966585\n",
            "Training log: 16 epoch (42368 / 60000 train. data). Loss: 0.002701215445995331\n",
            "Training log: 16 epoch (43648 / 60000 train. data). Loss: 0.04827972874045372\n",
            "Training log: 16 epoch (44928 / 60000 train. data). Loss: 0.05106446146965027\n",
            "Training log: 16 epoch (46208 / 60000 train. data). Loss: 0.0325479730963707\n",
            "Training log: 16 epoch (47488 / 60000 train. data). Loss: 0.006843590643256903\n",
            "Training log: 16 epoch (48768 / 60000 train. data). Loss: 0.008789658546447754\n",
            "Training log: 16 epoch (50048 / 60000 train. data). Loss: 0.019254297018051147\n",
            "Training log: 16 epoch (51328 / 60000 train. data). Loss: 0.03775807470083237\n",
            "Training log: 16 epoch (52608 / 60000 train. data). Loss: 0.018318170681595802\n",
            "Training log: 16 epoch (53888 / 60000 train. data). Loss: 0.00925864465534687\n",
            "Training log: 16 epoch (55168 / 60000 train. data). Loss: 0.0038298284634947777\n",
            "Training log: 16 epoch (56448 / 60000 train. data). Loss: 0.02635527402162552\n",
            "Training log: 16 epoch (57728 / 60000 train. data). Loss: 0.0038645928725600243\n",
            "Training log: 16 epoch (59008 / 60000 train. data). Loss: 0.014051178470253944\n",
            "Test loss (avg): 0.0680814693549648, Accuracy: 0.9806\n",
            "Training log: 17 epoch (128 / 60000 train. data). Loss: 0.007009311579167843\n",
            "Training log: 17 epoch (1408 / 60000 train. data). Loss: 0.006812115199863911\n",
            "Training log: 17 epoch (2688 / 60000 train. data). Loss: 0.004996421281248331\n",
            "Training log: 17 epoch (3968 / 60000 train. data). Loss: 0.0031195632182061672\n",
            "Training log: 17 epoch (5248 / 60000 train. data). Loss: 0.009375004097819328\n",
            "Training log: 17 epoch (6528 / 60000 train. data). Loss: 0.011575673706829548\n",
            "Training log: 17 epoch (7808 / 60000 train. data). Loss: 0.004543840419501066\n",
            "Training log: 17 epoch (9088 / 60000 train. data). Loss: 0.009766582399606705\n",
            "Training log: 17 epoch (10368 / 60000 train. data). Loss: 0.004349763039499521\n",
            "Training log: 17 epoch (11648 / 60000 train. data). Loss: 0.0034732746426016092\n",
            "Training log: 17 epoch (12928 / 60000 train. data). Loss: 0.011665929108858109\n",
            "Training log: 17 epoch (14208 / 60000 train. data). Loss: 0.003713002195581794\n",
            "Training log: 17 epoch (15488 / 60000 train. data). Loss: 0.0024457413237541914\n",
            "Training log: 17 epoch (16768 / 60000 train. data). Loss: 0.010172029957175255\n",
            "Training log: 17 epoch (18048 / 60000 train. data). Loss: 0.019544072449207306\n",
            "Training log: 17 epoch (19328 / 60000 train. data). Loss: 0.0027583339251577854\n",
            "Training log: 17 epoch (20608 / 60000 train. data). Loss: 0.007062884978950024\n",
            "Training log: 17 epoch (21888 / 60000 train. data). Loss: 0.011934006586670876\n",
            "Training log: 17 epoch (23168 / 60000 train. data). Loss: 0.008743277750909328\n",
            "Training log: 17 epoch (24448 / 60000 train. data). Loss: 0.003222591942176223\n",
            "Training log: 17 epoch (25728 / 60000 train. data). Loss: 0.014597617089748383\n",
            "Training log: 17 epoch (27008 / 60000 train. data). Loss: 0.006833492312580347\n",
            "Training log: 17 epoch (28288 / 60000 train. data). Loss: 0.003883310128003359\n",
            "Training log: 17 epoch (29568 / 60000 train. data). Loss: 0.003865121630951762\n",
            "Training log: 17 epoch (30848 / 60000 train. data). Loss: 0.020194798707962036\n",
            "Training log: 17 epoch (32128 / 60000 train. data). Loss: 0.0032088919542729855\n",
            "Training log: 17 epoch (33408 / 60000 train. data). Loss: 0.004627385176718235\n",
            "Training log: 17 epoch (34688 / 60000 train. data). Loss: 0.014699447900056839\n",
            "Training log: 17 epoch (35968 / 60000 train. data). Loss: 0.014628026634454727\n",
            "Training log: 17 epoch (37248 / 60000 train. data). Loss: 0.0070464336313307285\n",
            "Training log: 17 epoch (38528 / 60000 train. data). Loss: 0.006286873947829008\n",
            "Training log: 17 epoch (39808 / 60000 train. data). Loss: 0.007134691812098026\n",
            "Training log: 17 epoch (41088 / 60000 train. data). Loss: 0.014652898535132408\n",
            "Training log: 17 epoch (42368 / 60000 train. data). Loss: 0.038967885076999664\n",
            "Training log: 17 epoch (43648 / 60000 train. data). Loss: 0.009839225560426712\n",
            "Training log: 17 epoch (44928 / 60000 train. data). Loss: 0.0035195467062294483\n",
            "Training log: 17 epoch (46208 / 60000 train. data). Loss: 0.011328523978590965\n",
            "Training log: 17 epoch (47488 / 60000 train. data). Loss: 0.013495325110852718\n",
            "Training log: 17 epoch (48768 / 60000 train. data). Loss: 0.013647016137838364\n",
            "Training log: 17 epoch (50048 / 60000 train. data). Loss: 0.0053892601281404495\n",
            "Training log: 17 epoch (51328 / 60000 train. data). Loss: 0.010748849250376225\n",
            "Training log: 17 epoch (52608 / 60000 train. data). Loss: 0.007889150641858578\n",
            "Training log: 17 epoch (53888 / 60000 train. data). Loss: 0.008700386621057987\n",
            "Training log: 17 epoch (55168 / 60000 train. data). Loss: 0.012295873835682869\n",
            "Training log: 17 epoch (56448 / 60000 train. data). Loss: 0.008341251872479916\n",
            "Training log: 17 epoch (57728 / 60000 train. data). Loss: 0.0026344505604356527\n",
            "Training log: 17 epoch (59008 / 60000 train. data). Loss: 0.007090951316058636\n",
            "Test loss (avg): 0.06571141546368599, Accuracy: 0.9806\n",
            "Training log: 18 epoch (128 / 60000 train. data). Loss: 0.0021743513643741608\n",
            "Training log: 18 epoch (1408 / 60000 train. data). Loss: 0.009365463629364967\n",
            "Training log: 18 epoch (2688 / 60000 train. data). Loss: 0.004854131489992142\n",
            "Training log: 18 epoch (3968 / 60000 train. data). Loss: 0.005237582605332136\n",
            "Training log: 18 epoch (5248 / 60000 train. data). Loss: 0.002135793911293149\n",
            "Training log: 18 epoch (6528 / 60000 train. data). Loss: 0.0039315237663686275\n",
            "Training log: 18 epoch (7808 / 60000 train. data). Loss: 0.007553455885499716\n",
            "Training log: 18 epoch (9088 / 60000 train. data). Loss: 0.006312064826488495\n",
            "Training log: 18 epoch (10368 / 60000 train. data). Loss: 0.01711552031338215\n",
            "Training log: 18 epoch (11648 / 60000 train. data). Loss: 0.005231540184468031\n",
            "Training log: 18 epoch (12928 / 60000 train. data). Loss: 0.0033379981759935617\n",
            "Training log: 18 epoch (14208 / 60000 train. data). Loss: 0.00356785929761827\n",
            "Training log: 18 epoch (15488 / 60000 train. data). Loss: 0.0018577801529318094\n",
            "Training log: 18 epoch (16768 / 60000 train. data). Loss: 0.011273416690528393\n",
            "Training log: 18 epoch (18048 / 60000 train. data). Loss: 0.013759374618530273\n",
            "Training log: 18 epoch (19328 / 60000 train. data). Loss: 0.002936410019174218\n",
            "Training log: 18 epoch (20608 / 60000 train. data). Loss: 0.003731504315510392\n",
            "Training log: 18 epoch (21888 / 60000 train. data). Loss: 0.006449630483984947\n",
            "Training log: 18 epoch (23168 / 60000 train. data). Loss: 0.005421587731689215\n",
            "Training log: 18 epoch (24448 / 60000 train. data). Loss: 0.01044275052845478\n",
            "Training log: 18 epoch (25728 / 60000 train. data). Loss: 0.0009260624065063894\n",
            "Training log: 18 epoch (27008 / 60000 train. data). Loss: 0.0026943800039589405\n",
            "Training log: 18 epoch (28288 / 60000 train. data). Loss: 0.001963142305612564\n",
            "Training log: 18 epoch (29568 / 60000 train. data). Loss: 0.0467248298227787\n",
            "Training log: 18 epoch (30848 / 60000 train. data). Loss: 0.002002276014536619\n",
            "Training log: 18 epoch (32128 / 60000 train. data). Loss: 0.004530672449618578\n",
            "Training log: 18 epoch (33408 / 60000 train. data). Loss: 0.016117874532938004\n",
            "Training log: 18 epoch (34688 / 60000 train. data). Loss: 0.003930301405489445\n",
            "Training log: 18 epoch (35968 / 60000 train. data). Loss: 0.01509791985154152\n",
            "Training log: 18 epoch (37248 / 60000 train. data). Loss: 0.0015234104357659817\n",
            "Training log: 18 epoch (38528 / 60000 train. data). Loss: 0.01682981289923191\n",
            "Training log: 18 epoch (39808 / 60000 train. data). Loss: 0.005079797003418207\n",
            "Training log: 18 epoch (41088 / 60000 train. data). Loss: 0.009285538457334042\n",
            "Training log: 18 epoch (42368 / 60000 train. data). Loss: 0.004580537788569927\n",
            "Training log: 18 epoch (43648 / 60000 train. data). Loss: 0.0035430695861577988\n",
            "Training log: 18 epoch (44928 / 60000 train. data). Loss: 0.005155281629413366\n",
            "Training log: 18 epoch (46208 / 60000 train. data). Loss: 0.010441896505653858\n",
            "Training log: 18 epoch (47488 / 60000 train. data). Loss: 0.005965502932667732\n",
            "Training log: 18 epoch (48768 / 60000 train. data). Loss: 0.00423949072137475\n",
            "Training log: 18 epoch (50048 / 60000 train. data). Loss: 0.005569192580878735\n",
            "Training log: 18 epoch (51328 / 60000 train. data). Loss: 0.0052385758608579636\n",
            "Training log: 18 epoch (52608 / 60000 train. data). Loss: 0.016169635578989983\n",
            "Training log: 18 epoch (53888 / 60000 train. data). Loss: 0.003489213064312935\n",
            "Training log: 18 epoch (55168 / 60000 train. data). Loss: 0.0044914428144693375\n",
            "Training log: 18 epoch (56448 / 60000 train. data). Loss: 0.004369413945823908\n",
            "Training log: 18 epoch (57728 / 60000 train. data). Loss: 0.00217639422044158\n",
            "Training log: 18 epoch (59008 / 60000 train. data). Loss: 0.007648578844964504\n",
            "Test loss (avg): 0.0626665153414011, Accuracy: 0.983\n",
            "Training log: 19 epoch (128 / 60000 train. data). Loss: 0.003643629141151905\n",
            "Training log: 19 epoch (1408 / 60000 train. data). Loss: 0.013561191968619823\n",
            "Training log: 19 epoch (2688 / 60000 train. data). Loss: 0.006781770382076502\n",
            "Training log: 19 epoch (3968 / 60000 train. data). Loss: 0.004125566687434912\n",
            "Training log: 19 epoch (5248 / 60000 train. data). Loss: 0.0032636462710797787\n",
            "Training log: 19 epoch (6528 / 60000 train. data). Loss: 0.002510772319510579\n",
            "Training log: 19 epoch (7808 / 60000 train. data). Loss: 0.0046671172603964806\n",
            "Training log: 19 epoch (9088 / 60000 train. data). Loss: 0.0052885981276631355\n",
            "Training log: 19 epoch (10368 / 60000 train. data). Loss: 0.0011503625428304076\n",
            "Training log: 19 epoch (11648 / 60000 train. data). Loss: 0.000786646909546107\n",
            "Training log: 19 epoch (12928 / 60000 train. data). Loss: 0.004801139235496521\n",
            "Training log: 19 epoch (14208 / 60000 train. data). Loss: 0.016476571559906006\n",
            "Training log: 19 epoch (15488 / 60000 train. data). Loss: 0.00345975486561656\n",
            "Training log: 19 epoch (16768 / 60000 train. data). Loss: 0.0188444796949625\n",
            "Training log: 19 epoch (18048 / 60000 train. data). Loss: 0.0037956538144499063\n",
            "Training log: 19 epoch (19328 / 60000 train. data). Loss: 0.002999281045049429\n",
            "Training log: 19 epoch (20608 / 60000 train. data). Loss: 0.0019363162573426962\n",
            "Training log: 19 epoch (21888 / 60000 train. data). Loss: 0.00853068195283413\n",
            "Training log: 19 epoch (23168 / 60000 train. data). Loss: 0.014103730209171772\n",
            "Training log: 19 epoch (24448 / 60000 train. data). Loss: 0.00778166251257062\n",
            "Training log: 19 epoch (25728 / 60000 train. data). Loss: 0.009476002305746078\n",
            "Training log: 19 epoch (27008 / 60000 train. data). Loss: 0.0021949715446680784\n",
            "Training log: 19 epoch (28288 / 60000 train. data). Loss: 0.006997509393841028\n",
            "Training log: 19 epoch (29568 / 60000 train. data). Loss: 0.01577053591609001\n",
            "Training log: 19 epoch (30848 / 60000 train. data). Loss: 0.023006077855825424\n",
            "Training log: 19 epoch (32128 / 60000 train. data). Loss: 0.0072235604748129845\n",
            "Training log: 19 epoch (33408 / 60000 train. data). Loss: 0.005747364368289709\n",
            "Training log: 19 epoch (34688 / 60000 train. data). Loss: 0.002607642440125346\n",
            "Training log: 19 epoch (35968 / 60000 train. data). Loss: 0.0022335806861519814\n",
            "Training log: 19 epoch (37248 / 60000 train. data). Loss: 0.012092485092580318\n",
            "Training log: 19 epoch (38528 / 60000 train. data). Loss: 0.0032260471489280462\n",
            "Training log: 19 epoch (39808 / 60000 train. data). Loss: 0.028845982626080513\n",
            "Training log: 19 epoch (41088 / 60000 train. data). Loss: 0.0014087181771174073\n",
            "Training log: 19 epoch (42368 / 60000 train. data). Loss: 0.004252695944160223\n",
            "Training log: 19 epoch (43648 / 60000 train. data). Loss: 0.01163459476083517\n",
            "Training log: 19 epoch (44928 / 60000 train. data). Loss: 0.0039003670681267977\n",
            "Training log: 19 epoch (46208 / 60000 train. data). Loss: 0.006742712110280991\n",
            "Training log: 19 epoch (47488 / 60000 train. data). Loss: 0.0013944345992058516\n",
            "Training log: 19 epoch (48768 / 60000 train. data). Loss: 0.0016609624726697803\n",
            "Training log: 19 epoch (50048 / 60000 train. data). Loss: 0.0035007966216653585\n",
            "Training log: 19 epoch (51328 / 60000 train. data). Loss: 0.011716613546013832\n",
            "Training log: 19 epoch (52608 / 60000 train. data). Loss: 0.010741286911070347\n",
            "Training log: 19 epoch (53888 / 60000 train. data). Loss: 0.011448591947555542\n",
            "Training log: 19 epoch (55168 / 60000 train. data). Loss: 0.02249113842844963\n",
            "Training log: 19 epoch (56448 / 60000 train. data). Loss: 0.008177199400961399\n",
            "Training log: 19 epoch (57728 / 60000 train. data). Loss: 0.014998842030763626\n",
            "Training log: 19 epoch (59008 / 60000 train. data). Loss: 0.007533940486609936\n",
            "Test loss (avg): 0.06549651892855764, Accuracy: 0.9814\n",
            "Training log: 20 epoch (128 / 60000 train. data). Loss: 0.0014823260717093945\n",
            "Training log: 20 epoch (1408 / 60000 train. data). Loss: 0.0059269992634654045\n",
            "Training log: 20 epoch (2688 / 60000 train. data). Loss: 0.004224104806780815\n",
            "Training log: 20 epoch (3968 / 60000 train. data). Loss: 0.0019754739478230476\n",
            "Training log: 20 epoch (5248 / 60000 train. data). Loss: 0.0034336443059146404\n",
            "Training log: 20 epoch (6528 / 60000 train. data). Loss: 0.0019268426112830639\n",
            "Training log: 20 epoch (7808 / 60000 train. data). Loss: 0.006581940222531557\n",
            "Training log: 20 epoch (9088 / 60000 train. data). Loss: 0.0028516720049083233\n",
            "Training log: 20 epoch (10368 / 60000 train. data). Loss: 0.001996647333726287\n",
            "Training log: 20 epoch (11648 / 60000 train. data). Loss: 0.0029837852343916893\n",
            "Training log: 20 epoch (12928 / 60000 train. data). Loss: 0.0035960935056209564\n",
            "Training log: 20 epoch (14208 / 60000 train. data). Loss: 0.0016164758708328009\n",
            "Training log: 20 epoch (15488 / 60000 train. data). Loss: 0.0034812381491065025\n",
            "Training log: 20 epoch (16768 / 60000 train. data). Loss: 0.011076255701482296\n",
            "Training log: 20 epoch (18048 / 60000 train. data). Loss: 0.006775792222470045\n",
            "Training log: 20 epoch (19328 / 60000 train. data). Loss: 0.003434619400650263\n",
            "Training log: 20 epoch (20608 / 60000 train. data). Loss: 0.00532898772507906\n",
            "Training log: 20 epoch (21888 / 60000 train. data). Loss: 0.013596870936453342\n",
            "Training log: 20 epoch (23168 / 60000 train. data). Loss: 0.005098154302686453\n",
            "Training log: 20 epoch (24448 / 60000 train. data). Loss: 0.0066168950870633125\n",
            "Training log: 20 epoch (25728 / 60000 train. data). Loss: 0.0016436099540442228\n",
            "Training log: 20 epoch (27008 / 60000 train. data). Loss: 0.0015458574052900076\n",
            "Training log: 20 epoch (28288 / 60000 train. data). Loss: 0.0032778324093669653\n",
            "Training log: 20 epoch (29568 / 60000 train. data). Loss: 0.0013845430221408606\n",
            "Training log: 20 epoch (30848 / 60000 train. data). Loss: 0.0023059509694576263\n",
            "Training log: 20 epoch (32128 / 60000 train. data). Loss: 0.0030131046660244465\n",
            "Training log: 20 epoch (33408 / 60000 train. data). Loss: 0.0016111923614516854\n",
            "Training log: 20 epoch (34688 / 60000 train. data). Loss: 0.0021084826439619064\n",
            "Training log: 20 epoch (35968 / 60000 train. data). Loss: 0.0036605948116630316\n",
            "Training log: 20 epoch (37248 / 60000 train. data). Loss: 0.00713343545794487\n",
            "Training log: 20 epoch (38528 / 60000 train. data). Loss: 0.0025800506118685007\n",
            "Training log: 20 epoch (39808 / 60000 train. data). Loss: 0.0033782569225877523\n",
            "Training log: 20 epoch (41088 / 60000 train. data). Loss: 0.0016960485372692347\n",
            "Training log: 20 epoch (42368 / 60000 train. data). Loss: 0.019263699650764465\n",
            "Training log: 20 epoch (43648 / 60000 train. data). Loss: 0.00775039242580533\n",
            "Training log: 20 epoch (44928 / 60000 train. data). Loss: 0.00592435710132122\n",
            "Training log: 20 epoch (46208 / 60000 train. data). Loss: 0.0015443493612110615\n",
            "Training log: 20 epoch (47488 / 60000 train. data). Loss: 0.004232259001582861\n",
            "Training log: 20 epoch (48768 / 60000 train. data). Loss: 0.002978640142828226\n",
            "Training log: 20 epoch (50048 / 60000 train. data). Loss: 0.0015713715692982078\n",
            "Training log: 20 epoch (51328 / 60000 train. data). Loss: 0.011424032971262932\n",
            "Training log: 20 epoch (52608 / 60000 train. data). Loss: 0.001656687236391008\n",
            "Training log: 20 epoch (53888 / 60000 train. data). Loss: 0.0012753853807225823\n",
            "Training log: 20 epoch (55168 / 60000 train. data). Loss: 0.0036308791022747755\n",
            "Training log: 20 epoch (56448 / 60000 train. data). Loss: 0.007347107399255037\n",
            "Training log: 20 epoch (57728 / 60000 train. data). Loss: 0.010279577225446701\n",
            "Training log: 20 epoch (59008 / 60000 train. data). Loss: 0.004672426730394363\n",
            "Test loss (avg): 0.06613789813182083, Accuracy: 0.9809\n",
            "{'train_loss': [tensor(0.3323, grad_fn=<NllLossBackward>), tensor(0.1940, grad_fn=<NllLossBackward>), tensor(0.1053, grad_fn=<NllLossBackward>), tensor(0.0701, grad_fn=<NllLossBackward>), tensor(0.0891, grad_fn=<NllLossBackward>), tensor(0.0568, grad_fn=<NllLossBackward>), tensor(0.0555, grad_fn=<NllLossBackward>), tensor(0.0463, grad_fn=<NllLossBackward>), tensor(0.0180, grad_fn=<NllLossBackward>), tensor(0.0424, grad_fn=<NllLossBackward>), tensor(0.0592, grad_fn=<NllLossBackward>), tensor(0.0201, grad_fn=<NllLossBackward>), tensor(0.0116, grad_fn=<NllLossBackward>), tensor(0.0233, grad_fn=<NllLossBackward>), tensor(0.0043, grad_fn=<NllLossBackward>), tensor(0.0196, grad_fn=<NllLossBackward>), tensor(0.0112, grad_fn=<NllLossBackward>), tensor(0.0114, grad_fn=<NllLossBackward>), tensor(0.0068, grad_fn=<NllLossBackward>), tensor(0.0111, grad_fn=<NllLossBackward>)], 'test_loss': [0.25674401404857633, 0.1992448383331299, 0.15682375156879425, 0.1302484982252121, 0.10790070136785507, 0.09192169861644506, 0.08839927258491516, 0.08092695128023625, 0.07370994516387581, 0.07024191123247146, 0.0662525423616171, 0.06529531195606104, 0.06840249712541699, 0.0712637786924839, 0.06652733830176294, 0.0680814693549648, 0.06571141546368599, 0.0626665153414011, 0.06549651892855764, 0.06613789813182083], 'test_acc': [0.927, 0.9415, 0.9547, 0.9599, 0.9672, 0.9718, 0.9728, 0.9757, 0.9777, 0.9798, 0.9802, 0.9797, 0.9807, 0.9791, 0.9806, 0.9806, 0.9806, 0.983, 0.9814, 0.9809]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5dn/8c+VyUZWQjYIa9iTgCyGTcQNBEQFd9G6YLXUVp5qbf1VH7VV7GZrlS6IS9WnbnWtioqCIBSVNWBAdhIWCRASAmQl+/374xzIELJMkklmMrner9e8ZuZsc2UYvnPmvs+5jxhjUEop5bv8PF2AUkqp1qVBr5RSPk6DXimlfJwGvVJK+TgNeqWU8nH+ni6gtpiYGNOnTx9Pl6GUUu3Khg0bjhpjYuua53VB36dPH9LS0jxdhlJKtSsisr++edp0o5RSPk6DXimlfJwGvVJK+Tiva6NXSvmmiooKsrKyKC0t9XQp7VpwcDA9evQgICDA5XU06JVSbSIrK4vw8HD69OmDiHi6nHbJGENeXh5ZWVkkJia6vJ423Sil2kRpaSnR0dEa8i0gIkRHRzf5V5EGvVKqzWjIt1xz3kOfCfoTJeX8deluvsvK93QpSinlVXymjd7hJzyzdBcOPxjaI9LT5SillNfwmT368OAAekeHsPVQgadLUUp5oRMnTvDss882eb1p06Zx4sSJJq83a9Ys3nvvvSav1xp8JugBUhIiNOiVUnWqL+grKysbXG/RokV07ty5tcpqEz7TdAOQkhDJou+yyT9ZQWQn148xVUq1rcc/3so2N++UJSdE8JsrU+qd/+CDD5KZmcnw4cMJCAggODiYqKgoduzYwa5du7jqqqs4cOAApaWl3HvvvcyePRuoGX+rqKiIyy67jPPPP59Vq1bRvXt3PvroIzp16tRobcuWLeOXv/wllZWVjBo1igULFhAUFMSDDz7IwoUL8ff3Z/LkyTz11FO8++67PP744zgcDiIjI1m5cmWL3xsfC/oIALYdKmBcv2gPV6OU8iZ//OMf2bJlC+np6axYsYLLL7+cLVu2nD4e/eWXX6ZLly6cPHmSUaNGce211xIdfWaO7N69m3//+9+8+OKL3HDDDbz//vvccsstDb5uaWkps2bNYtmyZQwcOJDbbruNBQsWcOutt/LBBx+wY8cOROR089DcuXNZvHgx3bt3b1aTUV18LOitTtith/I16JXyYg3tebeV0aNHn3HS0d/+9jc++OADAA4cOMDu3bvPCvrExESGDx8OwLnnnsu+ffsafZ2dO3eSmJjIwIEDAbj99tuZP38+c+bMITg4mDvvvJMrrriCK664AoDx48cza9YsbrjhBq655hp3/Km+1UYfGx5EXHiQ238SKqV8T2ho6OnHK1asYOnSpaxevZpNmzYxYsSIOk9KCgoKOv3Y4XA02r7fEH9/f9atW8d1113HJ598wtSpUwF47rnn+O1vf8uBAwc499xzycvLa/ZrnH6tFm/By2iHrFKqLuHh4RQWFtY5Lz8/n6ioKEJCQtixYwdr1qxx2+sOGjSIffv2kZGRQf/+/Xnttde48MILKSoqoqSkhGnTpjF+/Hj69u0LQGZmJmPGjGHMmDF89tlnHDhw4KxfFk3lc0E/pHskK3cfpbSiiuAAh6fLUUp5iejoaMaPH8+QIUPo1KkT8fHxp+dNnTqV5557jqSkJAYNGsTYsWPd9rrBwcG88sorXH/99ac7Y++++26OHTvGjBkzKC0txRjD008/DcADDzzA7t27McYwceJEhg0b1uIaxBjT4o24U2pqqmnJFaY+33KYu1/fyIf3jGd4z/Z9SJRSvmT79u0kJSV5ugyfUNd7KSIbjDGpdS3vU230cGaHrFJKKR9suukR1YmIYH9tp1dKtYl77rmHb7755oxp9957L3fccYeHKjqbzwW9iJCsHbJKqTYyf/58T5fQKJ9rugEYkhDJjsMFVFZVe7oUpZTyOJ8M+pTuEZRVVpOZW+zpUpRSyuN8M+i1Q1YppU7zyaDvGxNKkL+fttMrpRQuBr2ITBWRnSKSISIP1jH/bhH5TkTSReRrEUl2mveQvd5OEZnizuLr4+/wI6lbhO7RK6VOa+549ADz5s2jpKSkwWX69OnD0aNHm7X91tZo0IuIA5gPXAYkAzc5B7ntTWPMUGPMcOBPwNP2usnATCAFmAo8a2+v1Z0aCsHbTghTSnlGawe9N3Pl8MrRQIYxZg+AiLwFzAC2nVrAGOPcRhIKnErXGcBbxpgyYK+IZNjbW+2G2huUkhDJG2u/58Cxk/SKDmntl1NKNcVnD0L2d+7dZtehcNkf653tPB79pZdeSlxcHO+88w5lZWVcffXVPP744xQXF3PDDTeQlZVFVVUVjz76KEeOHOHQoUNcfPHFxMTEsHz58kZLefrpp3n55ZcBuOuuu7jvvvvq3PaNN95Y55j07uZK0HcHDjg9zwLG1F5IRO4B7gcCgUuc1nUeHSjLnlZ73dnAbIBevXq5UnejTo1Nv/VQvga9UuqM8eiXLFnCe++9x7p16zDGMH36dFauXElubi4JCQl8+umngDXYWWRkJE8//TTLly8nJiam0dfZsGEDr7zyCmvXrsUYw5gxY7jwwgvZs2fPWdvOy8urc0x6d3PbCVPGmPnAfBG5GXgEuL0J674AvADWWDfuqGdQ13AcfsLWQwVcNrSbOzaplHKXBva828KSJUtYsmQJI0aMAKCoqIjdu3czYcIEfvGLX/CrX/2KK664ggkTJjR5219//TVXX3316WGQr7nmGr766iumTp161rYrKyvrHJPe3VzpjD0I9HR63sOeVp+3gKuaua7bBAc4GBAXph2ySqmzGGN46KGHSE9PJz09nYyMDO68804GDhzIxo0bGTp0KI888ghz585122vWte36xqR3N1eCfj0wQEQSRSQQq3N1ofMCIjLA6enlwG778UJgpogEiUgiMABY1/KyXZOcEMEWPcRSKcWZ49FPmTKFl19+maKiIgAOHjxITk4Ohw4dIiQkhFtuuYUHHniAjRs3nrVuYyZMmMCHH35ISUkJxcXFfPDBB0yYMKHObRcVFZGfn8+0adN45pln2LRpU6v87Y023RhjKkVkDrAYcAAvG2O2ishcIM0YsxCYIyKTgArgOHazjb3cO1gdt5XAPcaYqlb5S+qQkhDJfzYeJKewlLjw4LZ6WaWUF3Iej/6yyy7j5ptvZty4cQCEhYXx+uuvk5GRwQMPPICfnx8BAQEsWLAAgNmzZzN16lQSEhIa7YwdOXIks2bNYvTo0YDVGTtixAgWL1581rYLCwvrHJPe3XxuPHpna/bkMfOFNbxyxyguHhTnlm0qpZpHx6N3nw4/Hr2zZPvIG72GrFKqI/O5YYqdRQQH0Ds6hC0HtUNWKeUeY8aMoays7Ixpr732GkOHDvVQRY3z6aAH63j6LQd1j14pb2CMQUQ8XUaLrF271qOv35zmdp9uugGrQ/b7YyUUlFZ4uhSlOrTg4GDy8vJ0WJIWMMaQl5dHcHDTDi7x+T1653b6sX2jPVyNUh1Xjx49yMrKIjc319OltGvBwcH06NGjSev4fNDXDIWgQa+UJwUEBJCYmOjpMjokn2+6iQsPJi48iK3aIauU6qB8PuihZshipZTqiDpI0EeSkVtEaUWbnZSrlFJeo4MEfQRV1Yad2a6NVaGUUr6kgwT9qYuFa/ONUqrj6RBB37NLJ8KD/dmiQxYrpTqgDhH0IqIdskqpDqtDBD1YzTc7DhdQWVXt6VKUUqpNdaCgj6Csspo9R4s9XYpSSrWpDhT0VoesjmSplOpoOkzQ94sNJcjfT9vplVIdTocJen+HH4O7RejFwpVSHY5vBX15CVTVPxxxSkIE2w4V6DCpSqkOxXeCPi8T5g2BrR/Uu0hKQgQFpZVkHT/ZhoUppZRn+U7QRyVCaCx881eoZ499iHbIKqU6IJeCXkSmishOEckQkQfrmH+/iGwTkc0iskxEejvNqxKRdPu20J3Fn8HPD877GRzZAhnL6lxkUNdwHH6iHbJKqQ6l0aAXEQcwH7gMSAZuEpHkWot9C6QaY84B3gP+5DTvpDFmuH2b7qa66zb0eghPgG/m1Tk7OMBB/9gw7ZBVSnUoruzRjwYyjDF7jDHlwFvADOcFjDHLjTEl9tM1QNOuc+Uu/oEw7qew7ys4uKHORXQoBKVUR+NK0HcHDjg9z7Kn1edO4DOn58EikiYia0TkqrpWEJHZ9jJpLb6e5MjbISjSaquvQ3JCBDmFZeQUlrbsdZRSqp1wa2esiNwCpAJ/dprc2xiTCtwMzBORfrXXM8a8YIxJNcakxsbGtqyI4AgYdSdsW2gdiVPLkO46ZLFSqmNxJegPAj2dnvewp51BRCYBDwPTjTFlp6YbYw7a93uAFcCIFtTrmjF3gyMAVv39rFnJ9sXCt2nQK6U6CFeCfj0wQEQSRSQQmAmccfSMiIwAnscK+Ryn6VEiEmQ/jgHGA9vcVXy9wuNh2E2Q/iYU5ZwxKyI4gF5dQrRDVinVYTQa9MaYSmAOsBjYDrxjjNkqInNF5NRRNH8GwoB3ax1GmQSkicgmYDnwR2NM6wc9WIdaVpXD2ufPmqUdskqpjsTflYWMMYuARbWm/drp8aR61lsFDG1Jgc0W0x+SroD1L8L590FQ+OlZKQkRfLYlm4LSCiKCAzxSnlJKtRXfOTO2LuPvg9J82PjqGZNT7A5ZbadXSnUEvh30PVKh9/mwev4Zg52l2B2y2nyjlOoIfDvoAcbfCwUH4bv3Tk+KCw8mNjxIO2SVUh2C7wf9gEshLvmswc5ODVmslFK+zveDXsTaq8/dDru/OD05JSGC3TlFlFZUebA4pZRqfb4f9ABDroWIHmcMdjYkIZKqasPO7EIPFqaUUq2vYwS9IwDG3QP7v4ED64Gai4Vrh6xSytd1jKAHGHkbBHeGVdZgZz27dCI82F87ZJVSPq/jBH1QGIy6C7Z/Akd3IyIkd4tgi+7RK6V8XMcJerAHOws8PdhZSkIkOw4XUFlV7eHClFKq9XSsoA+LhRE/gE3/hsIjDOkeQVllNXuOFnu6MqWUajUdK+gBxs2B6kpYu8CpQ1bb6ZVSvqvjBX10P0iaDutfpl9EFUH+fmw9qO30Sinf1fGCHmD8z6AsH//01xjcNZwtukevlPJhHTPou58LfSbA6mcZ2jWEbYcKME7DIyillC/pmEEP1hj1hYe4XL6moLSSrOMnPV2RUkq1io4b9P0mQvwQRhz4F0K1dsgqpXxWxw16e7Cz4BMZTHKks0U7ZJVSPqrjBj1AytUQ2YufBS/SPXqllM/q2EFvD3Y2tGobflnrPF2NUkq1io4d9AAjb6XUP5Iby/9DbmGZp6tRSim3cynoRWSqiOwUkQwRebCO+feLyDYR2Swiy0Skt9O820Vkt3273Z3Fu0VgKLnJtzHZsYG9OzZ6uhqllHK7RoNeRBzAfOAyIBm4SUSSay32LZBqjDkHeA/4k71uF+A3wBhgNPAbEYlyX/nuEXnhPZw0gYRtWODpUpRSyu1c2aMfDWQYY/YYY8qBt4AZzgsYY5YbY0rsp2uAHvbjKcAXxphjxpjjwBfAVPeU7j4R0d1YFDCRgdmfQsFhT5ejlFJu5UrQdwcOOD3PsqfV507gs6asKyKzRSRNRNJyc3NdKMn90rv/AKiGFb/3yOsrpVRrcWtnrIjcAqQCf27KesaYF4wxqcaY1NjYWHeW5LL43oN5qfIy2Pgq7F/lkRqUUqo1uBL0B4GeTs972NPOICKTgIeB6caYsqas6w1SEiKZV3ktZaHd4eP7oLLc0yUppZRbuBL064EBIpIoIoHATGCh8wIiMgJ4Hivkc5xmLQYmi0iU3Qk72Z7mdVK6R3CSYJb1+xUc3Qnf/NXTJSmllFs0GvTGmEpgDlZAbwfeMcZsFZG5IjLdXuzPQBjwroiki8hCe91jwBNYXxbrgbn2NK8TFx7MoPhwXj06EJKvgpV/hrxMT5ellFItJt42PG9qaqpJS0vzyGv/6fMdPL9yD9/eN5SIl86DhOFw20JrXByllPJiIrLBGJNa1zw9M9bJxKQ4qqoNKw47YNJvYO9K2Py2p8tSSqkW0aB3MrxnFF1CA1m2/Qic+0PoMQoW/y+UeGVrk1JKuUSD3onDT7h4UBwrduZSaYAr/wql+bDkUU+XppRSzaZBX8vEpDjyT1awYf9xiE+BcXMg/XXY+5WnS1NKqWbRoK9lwoAYAhzCsh32UaIX/go694ZPfg6VOrqlUqr90aCvJTw4gLF9o612eoDAELj8acjbDV8/49nilFKqGTTo63DJ4Dgyc4vZd7TYmjBgEgy5Fr76Cxzd7dnilFKqiTTo6zBxcDwAS0/t1QNM+QMEdLKacLzs3AOllGqIBn0dekWHMCAujC93OI3mEB4Pkx6HfV9B+pueK04ppZpIg74eE5PiWbf3GAWlFTUTR94OPcfCkoeh+KjnilNKqSbQoK/HpKQ4KqsNK3c5jY/v5wdXzoOyQljyiOeKU0qpJtCgr8eIXlFEhQSwbHvOmTPikmD8vbDp37BnhUdqU0qpptCgr8eps2SX78yhsqr6zJkXPABRiVbHbEWpZwpUSikXadA3YGJSPCdKKvj2wIkzZwR0giuegWN7rEMulVLKi2nQN2DCwBj8/eTMwyxP6XcxnHOjdRJVzo62L04ppVykQd+AiOAAxvTtcnY7/SmTfweBofDJfVBdXfcySinlYRr0jZg4OJ6MnCL25xWfPTMsFiY/Ad+vhm9fa/vilFLKBRr0jZiYFAdQ/179iFuh93j44lEoqmcZpZTyIA36RvSODqV/XBjLdtTRTg/WZQaveAbKS2Dxw21bnFJKuUCD3gUTB8exds8xCp3PknUWOwgm3A/fvQPbP2nb4pRSqhEa9C6YmBRvnyXbwLAH598P3YbBe3fA7i/arjillGqES0EvIlNFZKeIZIjIg3XMv0BENopIpYhcV2telYik27eF7iq8LY3s1ZnOIQH1N98ABATDrR9C7GB462YNe6WU12g06EXEAcwHLgOSgZtEJLnWYt8Ds4C6hnU8aYwZbt+mt7Bej/B3+HHRwFhW7MylqrqBIYpDusBtH2nYK6W8iit79KOBDGPMHmNMOfAWMMN5AWPMPmPMZsBnDyafmBTPseJyvv3+eMMLatgrpbyMK0HfHTjg9DzLnuaqYBFJE5E1InJVXQuIyGx7mbTc3Ny6FvG4CwbG4u/ndC3ZhmjYK6W8SFt0xvY2xqQCNwPzRKRf7QWMMS8YY1KNMamxsbFtUFLTRXYKYFSfLjXXkm2Mhr1Syku4EvQHgZ5Oz3vY01xijDlo3+8BVgAjmlCfV5mYFMeuI0UcOFbi2goa9kopL+BK0K8HBohIoogEAjMBl46eEZEoEQmyH8cA44FtzS3W0yYlWdeSdXmvHjTslVIe12jQG2MqgTnAYmA78I4xZquIzBWR6QAiMkpEsoDrgedFZKu9ehKQJiKbgOXAH40x7Tbo+8SE0jc21LV2emenwj4uScNeKdXmxJgGDhf0gNTUVJOWlubpMur1+0XbeeWbvXz768mEBfk3beWSY/DaVZCzHWa+CQMubZ0ilVIdjohssPtDz6JnxjbRJYPjqKgyfLWrGUcHhXSxTqrSPXulVBvSoG+i1N5RRHYKYGl9o1k2RsNeKdXGNOibyN/hx0WDYlmxM6fhs2QbomGvlGpDGvTNcMngOPKKy0mvfS3ZptCwV0q1EQ36ZrhoYBwOP2naYZZ10bBXSrUBDfpmiAwJYFSfKL5s6mGWdakd9ts/bvk2lVLKiQZ9M00cHM+O7EKyjrt4lmxDToV913Pg7Vvh63ngZYe9KqXaLw36Zjp1LVm37NWDFfazPoGUq2Dpb+CjOVBZ7p5tK6U6NA36ZuobG0bfmNDmH2ZZl4BOcN0rcOGDkP66dXJVcZ77tq+U6pA06FvgksFxrMnMo6is0n0bFYGLH4JrX4KsNPjnJZC7033bV0p1OBr0LTAxKZ7yqmq+3t3AtWSba+h1MOtTKC+Bf14KGcvc/xpKqQ5Bg74FUvtEER7s3/LDLOvTcxT86Evo3BPeuB7Wvdg6r6OU8mka9C0Q4PDjokFxLN+ZQ3Vzz5JtTOee8MPPrQHQFv0SFj0AVW5sKlJK+TwN+haalBTH0aJyNmW14CzZxgSFW6NdjpsD616AN2+A0vzWez2llE/RoG+hCwfG2mfJuvHom7r4OWDK72D632Hvf612+2N7W/c1lVI+QYO+hTqHBHJu7yiWtlY7fW0jb7NOrirOgRcvgf2r2uZ1lVLtlga9G0xKimNHdiEHT5xsmxdMnAB3LbNOsvrXdPj2jbZ5XaVUu6RB7waXDLauJftlW+3VA0T3g7uWQu/z4KOfwhe/gerqtnt9pVS7oUHvBv1iQ+kTHdL0a8m2VKcouOV9SP0hfDMP3rkVyoratgallNfToHcDEWFiUjyrMvMoKW/jQx8dAXD50zD1Sdi5COaPgW0f6aBoSqnTNOjdZOLgOMorq1mxsxnXkm0pERh7N9zxubWX/85t8Pq1cDSj7WtRSnkdl4JeRKaKyE4RyRCRB+uYf4GIbBSRShG5rta820Vkt3273V2Fe5tRiV3o1SWE336yjePFHhp1stcYmL0CLvsTZK2HBeNg2VwoL/ZMPUopr9Bo0IuIA5gPXAYkAzeJSHKtxb4HZgFv1lq3C/AbYAwwGviNiES1vGzvE+Dw4x83j+BoUTn3v5PeemfKNsbhD2N+DHPSYMi18NVfrOac7R9rc45SHZQre/SjgQxjzB5jTDnwFjDDeQFjzD5jzGag9mEfU4AvjDHHjDHHgS+AqW6o2yud06Mzj16RxPKduTy/co9niwmPh6ufgzs+g6AIePsWeOM6yMv0bF1KqTbnStB3Bw44Pc+yp7nCpXVFZLaIpIlIWm6uB9q43eiWsb254pxuPLVkJ2v3eMFY8r3Pgx+vhKl/hAPr4Nmx8OVvrVExlVIdgld0xhpjXjDGpBpjUmNjYz1dTouICH+4Zii9uoTwP//+lqNFZZ4uyWrOGfsTqzkn5RpY+WerOWfHp9qco1QH4ErQHwR6Oj3vYU9zRUvWbbfCgwOYf/NI8k9WcN9b6VR5qr2+tvB4uOZ5mLUIgsKsi5G/eYM25yjl41wJ+vXAABFJFJFAYCaw0MXtLwYmi0iU3Qk72Z7m85ITIpg7I4WvM47yjy+97DDHPuOt5pwpf4D9q+3mnN9pc45SPsq/sQWMMZUiMgcroB3Ay8aYrSIyF0gzxiwUkVHAB0AUcKWIPG6MSTHGHBORJ7C+LADmGmOOtdLf4nVuSO3J2j3HmLdsF6l9ohjfP8bTJdVwBMC4n8KQa2DJo7DyT7D5LatpJ6pPzS2yh7WsUqrdEuNlbbSpqakmLS3N02W4TUl5JTP+8Q3HS8r59GcTiI8I9nRJddv3NXzxazi8GaoraqaLAyK7nxn+p2+J1glaIh4pWSlVQ0Q2GGNS65ynQd/6dh8pZPo/vmFoj0jevGsM/g7394FvzjrBYwu3ckNqT2aO7tX8DVVXQeFhOL6v7ltxraOigiIgqndN+HdPhb4XQafOza9BKdVkGvRe4INvs/j525u45+J+PDBlsNu2W1VteO6/mTzzxS5O/Uu++sPRrddMVFYEJ76v/4ugqsz6FdBjFPSfBP0nQrfh4OcVB3gp5bM06L3Eg+9v5q31B3jljlFcPCiuxds7dOIkP387nbV7j3H5Od14eFoSs15Zx5GCMj68ZzyJMaFuqLoJqirhYBpkLLVuh761podEQ7+JVvD3uwTC2vchtEp5Iw16L1FaUcVV878hu6CURT+bQELnTs3e1iebD/G///mOqmrD4zOGcO3I7ogI3+eVMGP+13QJDeSDe8YTEezBjtSiXNiz3A7+ZVBy1Jrebbi9tz/J2vN3NHpMgFKqERr0XmRPbhHT//ENA+PDePvH4whoYnt9UVklv/loK+9vzGJ4z878deZwekefuee+OjOPW19ay/kDYnjp9lE4/Lygs7S6GrI31YT+gXVgqiAoEvpeWNPME9nD05Uq1S5p0HuZTzYfYs6b3/KjCYk8fHnt8eHqt/H749z3VjpZx0uYc3F//mfigHq/KF5fs59HPtzC7Av68r/TktxVuvucPAF7V0LGF1bwF9jn0cUMspp3+l0MvcdbJ3YppRrVUNDrb2YPuOKcBNbtPcaLX+1lVJ8uTE7p2uDyVdWG+csz+Ouy3XSNCObtH49jVJ8uDa5zy9je7Mwu5IWVexgUH86153rZnnKnzpA83boZA7k7rL39zOWw4RVYuwD8AqDnGCv0+11sd+o6PF25Uu2O7tF7SFllFdctWM2+vGIW/WwCPbuE1LncgWMl3P9OOuv3HWfG8ASeuGqIy+3uFVXV3PbSOjbsP85bPx7LyF7tZIToilI4sAYyv7SCP3uzNb1TFCReWLPH37kFh5Eq5WO06cZLHThWwrS/fUViTCjv3j2OIP8z91Y/Sj/IIx9sAeCJq4Zw1QhXBw2tcby4nBnzv6GkvIqP/2c83SKb3wHsMUW5sPe/dvB/aR3nDxDdH/pebAV/n/MhOMKzdSrlQRr0XmzJ1mxmv7aB28f15vEZQwAoKK3g1x9u4cP0Q5zbO4p5Nw6vd4/fFbuOFHLNs6voExPCuz8+j06B7bj5wxjI3WkF/p7l1hm9FSXg528dwdNtGHQ+dQJXb+uxtvOrDkCD3sv99pNt/PPrvfzj5hF0jQjmvrfTOZxfyr0TB/DTi/q55UzaZduPcNeraVw+tBt/v2kE4ivDFlSWWUfwZH4Je1bA0V1QXnTmMiExNaF/6gsgqo/1XMfysVSUWr+UgsIh1IvGZFIu06D3chVV1dz4/Gq2HS6gvLKa7lGdmHfjCM7t7d429QUrMnny8x384tKB/M/EAW7dttcwBkqOWWfpntgHx/fbj+37/CyorqxZ/tRYPp17W18AXfpZTULR/aFLXwjw0rGJXGUMlBVAwaGaW+Fh6yingsP280NQ4nSRnLB4iEuG+JSaW8yg9v9e+DgN+nbg0ImT3PjCakb3ieax6cmEt8KJTsYYfv52Oh+mH+K5W85l6pCGj/bxSVWVVsid2H/2l18f+ccAABidSURBVMBZY/kIdO4J0QOs4I8ZANH2F0FED+8Y1qE03/ryys+C/AOQf/DsIK+o4+LwITEQ0Q0iukO4fR/Rzdreka1wZAvk7LCGtADrCzG6vx38yRA/xPoy6Nyr+YPaVVdDeSGUFlhfRqUF1q+r0FgIi4OAdtif5EEa9O2EMabVm1RKK6q48fnV7M4p4v2fnEdSN+3APENpAeRlWBdjycuAvN01z52bhPyDrb3/mP41vwCiB0BoNASEWPMDOoEjsPlBWFVhBfUZQZ5Vcys4aAWkM3HYwZ1QK8gTam7h3cA/yIXXr4Rje+zQ31bzBXDi+5plgiIgLsn6AohLtp6XFVhfGKfCu6zQ6bHTfVkh0ED+BNrNSGFxNeEfGmcNoREaaz+25wWFt89RVI2x/p2ryqx7gJCGD52ujwa9OsORglKm/+Nr/P38WDhnPNFhLvyn7+iMgcJsp/DPhKP2l8DxfdZZvnUSK/gDgsG/k3Uf0Knm8elp9peD8x564WHOCsKQaKtfIbKnFeKRPWqeR3a3ml1a+1yD0gLI2Q45W+3w3wpHtkFZ/pnLOQKt4A+OqHUfad0HhZ89r6rC+lVVnGMdbVWcA0U5UHzUelxy7Oz3BKz3LjTOuopaZI8z35uI7tb7Exrjni+Dqgrrs+D8y6nwkHVfmg9V5XZ4l9cEuPO0SqdpzkOCg3VAwV1Lm1WWBr06y6YDJ7jh+dUM69GZ1+8aQ6C/FzRDtFdVFVbY52XAyeNQcRIqS62jgSpK7ccna+4rTkLlSXuefX9qWlC4U3D3ODPII7pDYPOPvmpVxlhfTpVlNaHdGm36VZXWmElFOVbwFzs9Lsq1wvfUr53K0jPXdQRZv2jO+AKo9RjsAK/Vl3FqWsEhu3mvVm76B1u/lDp1tl7HEWD9anIEWo8dgXU8rmNaRDdIntGst0aDXtXpo/SD3PtWOjeN7snvrx7qO0fiKGWM1cF8KvSdm7vys2r6Mur9JeakUxSEJ9Q0h4XX0SzmBRfg0SEQVJ1mDO/OzuxCnl2RyaD4cGaNT/R0SUq5h4jVVBMaAwnD616mqhKKsq3Qzz9QM97S6SC3+zN8oFNYg76D++XkQew6UsgTn26nf1w45w/QY6ibI/3ACX77yTYuGhTLnEt89NBVX+Pwr2m6YYynq2lVGvQdnJ+fMG/mCK559hvueXMj9186kPBgf0KD/AkL8ick0EFYkPU8NNCf0CBHq1wKsb0qKqvkqcU7+dfqffj7CRu+P87YvtGkNjLonFJtSdvoFQDf55Vw/fOrOFJQ1uiygf5+dvg77PA/9cXgYHjPzlxxTkKLLqrSXizddoRHP9pCdkEpt47tzU8v6s/1z6/CT4TP7p1ASKDuR6m20+LOWBGZCvwVcAD/NMb8sdb8IOBV4FwgD7jRGLNPRPoA24Gd9qJrjDF3N/RaGvSeU15ZzYmT5ZSUVVFUVklJeRXFZZX240qKyqznxeWV1v0Zz6vIP1nB3qPWyTmj+kRx5bAELhvSjdhw3zp8M6eglMc+3sqi77IZFB/O768Zevos5jV78rjpxTXcOrY3c+2xi5RqCy3qjBURBzAfuBTIAtaLyEJjzDanxe4Ejhtj+ovITOBJ4EZ7XqYxpp7eEOVNAv39iAsPhvDmb2Pf0WI+2XyIjzcd5tcfbeWxhVsZ3z+GK89JYEpKVyJD2u+4MtXVhjfXfc+Tn++grLKaB6YM4kcT+p5xaOrYvtHccV4iL3+zl8nJXbXPQ3mFRvfoRWQc8JgxZor9/CEAY8wfnJZZbC+zWkT8gWwgFugNfGKMcXnXRvfofcfO7EI+3nSIjzcfYn9eCQEO4cKBsVw5LIFJSfGEBrWfpo1dRwp56D/fsWH/cc7rF83vrh5a78XXSyuqmPa3rygtr+Lzn1/g2ev2qg6jpYdXdgcOOD3P4uwu6tPLGGMqRSQfiLbnJYrIt0AB8Igx5qs6CpwNzAbo1UsvJuErBnUNZ1DXQfxi8kC+O5jPwvRDfLL5MEu35xAc4MfEpHiuPCeBiwbFEhzgnUMnl1ZU8ezyDBb8N5PQIH+eun7Y6Qux1yc4wMFfrh/GtQtW8cTH2/jz9cPasGKlztbau1SHgV7GmDwRORf4UERSjDFnDNBhjHkBeAGsPfpWrkm1MRHhnB6dOadHZ/53WhJp+4+zcNNBFn2XzaebDxMe5M/klK5cOawbEwbEesfFzLHa2//3P9+x52gxV4/oziOXJ7k8XMSIXlH85KJ+zF+eyZSUrkxKjm/lapWqnytBfxDo6fS8hz2trmWy7KabSCDPWO1CZQDGmA0ikgkMBLRtpoPy8xNGJ3ZhdGIXHrsyhVWZeSzcdIjFW7J5f2MWEwfH8dyt59Z70fO2cKKknD8s2sHbaQfo1SWE1+4czYQBsU3ezs8mDmDZ9hwe/M93fNE7iqjQwFaoVqnGufK/aT0wQEQSRSQQmAksrLXMQuB2+/F1wJfGGCMisXZnLiLSFxgA7HFP6aq983f4ccHAWJ66fhjrH5nEw9OSWLYjh1+8s4mq6rb/YWeMYeGmQ0x6+r+8tzGLuy/sx+L7LmhWyAME+Tt4+obh5J8s59GPtri5WqVc1+gevd3mPgdYjHV45cvGmK0iMhdIM8YsBF4CXhORDOAY1pcBwAXAXBGpAKqBu40xx1rjD1HtW3CAgx9d0JfKasOTn+8gPNif3141pM3G3ykuq+Tet9JZuv0Iw3pE8uoPx5Cc0PIhnJMTIrh34gCeWrKLKSmHuHJYghuqVapp9IQp5XWe/HwHC1Zk8pOL+vGrqYNb/fWOF5cz6//Ws+VgPg9dNpg7xie6tZ+gsqqaaxesYv+xEpb8/ALrEFal3Kyho270XHbldf7flEH8YEwvFqzI5NkVGa36WofzT3L986vZfriABT8YyV0T+rq9M9jf4cdfbhjOyfIqHnr/O7xt50r5Pg165XVEhCdmDGHG8AT+9PlOXl+zv1VeJzO3iOsWrCY7v5RXfziaySmtd2nF/nFhPDBlEMt25PDehqxWex2l6qJBr7ySn5/w1PXDmDg4jkc/2sJH6bUP9GqZ77Lyuf651ZRVVvHW7LGM7Rvd+Eot9MPxiYxO7MLcj7dx8MTJVn89pU7RoFdeK8Dhx/wfjGRMYhfuf2cTS7cdcct2V2UcZeYLqwkJdPDu3ecxpHukW7bbGD8/4anrhlFlDL96bzPVHjiySHVMGvTKqwUHOPjn7aMYkhDBT9/cyKrMoy3a3udbDjPrlfX0iArh/Z+cV+8wBq2lV3QID1+exNcZR3ljbes0SSlVmwa98nphQf783x2j6d0lhB/9K430AyeatZ231n3PT9/YyJDuEbz947HER3jm6JebR/diwoAYfr9oB/vs0T6Vak0a9KpdiAoN5PW7xtAlLJBZr6xjZ3ahy+saY1iwIpMH//MdEwbE8vpdY+gc4rmzVEWEP113Dv4O4ZfveubkMNWxaNCrdiM+Ipg37hxLoMOPW19ay/68xveGjTH8ftF2nvx8B9OHJfDibalecUGQbpGdeHx6Cmn7j/PS13qyuGpdGvSqXekVHcLrd42hvKqaW15aS3Z+ab3LVlZV88t3N/PiV3u5fVxv5t04/Iyx4z3t6hHdmZwcz1OLd7HriOu/UJRqKu/51CvlooHx4fzrjtEcKyrn1pfWcqy4/KxlSiuquPv1jby/MYv7Jg3gsekp+HnJqJiniAi/u3ooYcH+/OKdTVRUVXu0ngPHSjhRcvZ7qdo/DXrVLg3r2Zl/3j6K/cdKmPXKOgpLK07PKyit4LaX17FsxxHmzkjhvkkD22zMnKaKDQ/it1cN4buD+Ty7PLPNX7+62rBiZw63vrSWCX9azkVPreDzLdltXodqXRr0qt0a1y+aBT8YybZDBdz1rzRKK6rILSxj5vNr2Lj/OPNuHM5t4/p4usxGTRvajenDEvj7l7tZlXG0TYZIOFlexRtr9zN53kpmvbKendmF3DdpAL26hHD36xt48P3NlJRXtnodqm3ooGaq3fso/SD3vZ3O+f1jOHCshCMFZSy4ZSQXDYrzdGkuO1FSztR5X5FdUEqvLiFMHdKVKSldGdGzs1ubnHIKSnl19X7eWLuf4yUVDOkewZ3nJ3L50AQC/f0or6zmmaW7eO6/mSRGh/LXmSMY2qN1TygzxrBsew5Pfr6DkvIqLk2OZ3JKPKP7dMHfg9claG8aGtRMg175hNfX7OeRD7cQ2SmAl2eN4tzeUZ4uqcmOF5fz+dZsPt+SzarMo1RUGeLCg5iS0pXLhnRldGLzg2/LwXxe/novH28+RGW14dKkeO483xqSoa5mrVWZR7n/7U3kFZfxi8mDmD2hb6v0cezMLuS3n27jq91H6RcbSmJMGF/tzqWsspqokAAmJsUzJaUrEwbEeO3lJr2FBr3qEJbvzCExOpQ+bXy2a2vIP1nB8h05fL4lmxW7ciitqKZzSACXJsUzdUhXxvdvPPiqqg3Lth/hpa/3snbvMUIDHVyf2pM7xvehd3Tj79GJknIe+s93fLYlm/P6RfP0DcPpGumek8yOFZfzzBe7eGPtfsKC/Pn5pQO5ZWxvAhx+FJdVsnJXLou3ZrNsRw6FpZWEBDq4cGAsU1K6cvHgOCI76QXXa9OgV6odKym3gu/zLdks255DYVklYUH+XDw4jqkpXbloUCyhQTXnBhSXVfJu2gFeWbWP/XkldO/ciVnn9eGGUT2bHJDGGN5JO8BjC7cRFODHH685h6lDmj/KZ0VVNa+t3s+8pbsoLq/iB2N68fNJA+u9zGJ5ZTVr9uSxeGs2X2w7Qk5hGf5+wrh+0UxJ6crk5HjiPHSGs7fRoFfKR5RXVrMq8yiLt2azZOsR8orLCfT344IBsUxOiScjp4h/r/uewtJKRvTqzJ3nJzI1pWuL27r35BZx39vpbM7KZ+aonvz6yuQmn3i2fEcOT3y6jT25xUwYEMOjVyQzMD7c5fWrqw3fHjjBkm3W3773aDEiMKJnZ6akWH0atX/NGWMwBqqNocp+XFVtqDaG6uqa6aeeR3YKoFOg55qIjDHNPkJMg14pH1RVbUjbd4zPtmSzeGs2h/NLcfgJU4d05c7zExnZy739FM3tqM3IKeSJT7bz3125JMaE8sjlSVwyOK5Fh7waY9idU8TiLdks3pbNloMFAAQH+FFtrC+FamNo6ugSItCrSwiD4sMZ1DWcgfZ9YkyoWy9YX1pRRWZuERk5RWTmFJFhP+7euROv3DG6WdvUoFfKxxlj2Ha4gC6hgXSL7NSqr+VqR+2JknLmLd3Na2v2ExLo4N6JA7htXJ9WOTs563gJX2w7QnZ+KSKCn4DDTxARHPZzPz/BTwSHH/jJqXk10/1EyC0sY9eRQnYeKWTv0eLT4xAFOIS+MWEM6ur0BRAfTo+oTg12Up8oKScjp6jmZgf6wRMnORW9fvaXS/+4MEb2juKnF/Vv1nugQa+UcquGOmorq6p5Y+33PLN0FwUnK7hpdC/uv3Qg0WFBHq66acoqq8jMKT4d/Luyrfus4zUXjekU4GBgfNjp8PcTOR3me3KLOFpUc6ZxkL8ffWPD6B8XRv9T93Fh9IkJIci/5c1FGvRKKberq6M2JNDBE59sY3dOEef1i+bRK5JJ6hbh6VLdqrC0gt05RaeDf9eRQnZmF3G0qAyw2vlrh3m/2DC6R3Vy+/WInbU46EVkKvBXwAH80xjzx1rzg4BXgXOBPOBGY8w+e95DwJ1AFfAzY8zihl5Lg16p9sW5oxagd3QID09L4tLkeK8deqI15BWVUW0gJizQI393Q0HfaLe5iDiA+cClQBawXkQWGmO2OS12J3DcGNNfRGYCTwI3ikgyMBNIARKApSIy0BhT1bI/SSnlLfrGhvHe3efxwspMggMc3Dqut1uaItobb26acuX4qNFAhjFmD4CIvAXMAJyDfgbwmP34PeAfYn2lzQDeMsaUAXtFJMPe3mr3lK+U8gaB/n7MuWSAp8tQ9XCl+7s7cMDpeZY9rc5ljDGVQD4Q7eK6SimlWpFXjBgkIrNFJE1E0nJzcz1djlJK+RRXgv4g0NPpeQ97Wp3LiIg/EInVKevKuhhjXjDGpBpjUmNjY12vXimlVKNcCfr1wAARSRSRQKzO1YW1llkI3G4/vg740liH8ywEZopIkIgkAgOAde4pXSmllCsa7Yw1xlSKyBxgMdbhlS8bY7aKyFwgzRizEHgJeM3ubD2G9WWAvdw7WB23lcA9esSNUkq1LT1hSimlfEBDx9F7RWesUkqp1qNBr5RSPs7rmm5EJBfY7+k6GhADHPV0EQ3Q+lpG62sZra9lWlJfb2NMnYctel3QezsRSauvHcwbaH0to/W1jNbXMq1VnzbdKKWUj9OgV0opH6dB33QveLqARmh9LaP1tYzW1zKtUp+20SullI/TPXqllPJxGvRKKeXjNOhrEZGeIrJcRLaJyFYRubeOZS4SkXwRSbdvv/ZAnftE5Dv79c8aM0IsfxORDBHZLCIj27C2QU7vTbqIFIjIfbWWadP3UEReFpEcEdniNK2LiHwhIrvt+6h61r3dXma3iNxe1zKtVN+fRWSH/e/3gYh0rmfdBj8LrVjfYyJy0OnfcFo9604VkZ32Z/HBNqzvbafa9olIej3rtsX7V2eutNln0BijN6cb0A0YaT8OB3YBybWWuQj4xMN17gNiGpg/DfgMEGAssNZDdTqAbKyTOTz2HgIXACOBLU7T/gQ8aD9+EHiyjvW6AHvs+yj7cVQb1TcZ8LcfP1lXfa58FlqxvseAX7rw758J9AUCgU21/z+1Vn215v8F+LUH3786c6WtPoO6R1+LMeawMWaj/bgQ2E77vCrWDOBVY1kDdBaRbh6oYyKQaYzx6NnOxpiVWCOrOpsB/Mt+/C/gqjpWnQJ8YYw5Zow5DnwBTG2L+owxS4x1xTaANVjXc/CIet4/V5y+FKkxphw4dSlSt2qoPvuypjcA/3b367qqgVxpk8+gBn0DRKQPMAJYW8fscSKySUQ+E5GUNi3MYoAlIrJBRGbXMd9bLuM4k/r/g3n6PYw3xhy2H2cD8XUs4y3v4w+xfqHVpbHPQmuaYzctvVxPs4M3vH8TgCPGmN31zG/T969WrrTJZ1CDvh4iEga8D9xnjCmoNXsjVlPEMODvwIdtXR9wvjFmJHAZcI+IXOCBGhok1oVqpgPv1jHbG97D04z1G9krjzUWkYexrufwRj2LeOqzsADoBwwHDmM1j3ijm2h4b77N3r+GcqU1P4Ma9HUQkQCsf4w3jDH/qT3fGFNgjCmyHy8CAkQkpi1rNMYctO9zgA+wfiI7c+kyjq3sMmCjMeZI7Rne8B4CR041Z9n3OXUs49H3UURmAVcAP7CD4CwufBZahTHmiDGmyhhTDbxYz+t6+v3zB64B3q5vmbZ6/+rJlTb5DGrQ12K3570EbDfGPF3PMl3t5RCR0VjvY14b1hgqIuGnHmN12m2ptdhC4Db76JuxQL7TT8S2Uu+elKffQ5vzJTBvBz6qY5nFwGQRibKbJibb01qdiEwF/h8w3RhTUs8yrnwWWqs+5z6fq+t5XVcuRdqaJgE7jDFZdc1sq/evgVxpm89ga/Y0t8cbcD7Wz6fNQLp9mwbcDdxtLzMH2Ip1BMEa4Lw2rrGv/dqb7Doetqc71yjAfKwjHr4DUtu4xlCs4I50muax9xDrC+cwUIHVxnknEA0sA3YDS4Eu9rKpwD+d1v0hkGHf7mjD+jKw2mZPfQ6fs5dNABY19Floo/pesz9bm7ECq1vt+uzn07COMslsy/rs6f936jPntKwn3r/6cqVNPoM6BIJSSvk4bbpRSikfp0GvlFI+ToNeKaV8nAa9Ukr5OA16pZTycRr0SrmRWKNyfuLpOpRypkGvlFI+ToNedUgicouIrLPHIH9eRBwiUiQiz9jjhS8TkVh72eEiskZqxoWPsqf3F5Gl9sBsG0Wkn735MBF5T6yx5N84dQawUp6iQa86HBFJAm4ExhtjhgNVwA+wzuZNM8akAP8FfmOv8irwK2PMOVhngp6a/gYw31gDs52HdWYmWCMT3oc13nhfYHyr/1FKNcDf0wUo5QETgXOB9fbOdieswaSqqRn86nXgPyISCXQ2xvzXnv4v4F17fJTuxpgPAIwxpQD29tYZe2wV+6pGfYCvW//PUqpuGvSqIxLgX8aYh86YKPJoreWaOz5ImdPjKvT/mfIwbbpRHdEy4DoRiYPT1+3sjfX/4Tp7mZuBr40x+cBxEZlgT78V+K+xrhKUJSJX2dsIEpGQNv0rlHKR7mmoDscYs01EHsG6qpAf1oiH9wDFwGh7Xg5WOz5Yw8c+Zwf5HuAOe/qtwPMiMtfexvVt+Gco5TIdvVIpm4gUGWPCPF2HUu6mTTdKKeXjdI9eKaV8nO7RK6WUj9OgV0opH6dBr5RSPk6DXimlfJwGvVJK+bj/D9q7l4de8gZfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaof4a5r4ozN"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as f\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTXT0LZnvzyQ"
      },
      "source": [
        "N = 10\n",
        "L = 5"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHQHxCwO5cNW",
        "outputId": "06671c66-4c5f-4d48-b254-a4985d753307"
      },
      "source": [
        "S0 = torch.sign(torch.randn(9,N))\n",
        "\n",
        "SL = torch.sign(torch.randn(9,N))\n",
        "\n",
        "\n",
        "M = len(S0)\n",
        "print(M)\n",
        "print(N)\n",
        "print(S0.shape)\n",
        "print(S0[0].shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "10\n",
            "torch.Size([9, 10])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGa68qZ85cPY",
        "outputId": "43df1073-d57d-42a4-cde5-66e679e0a7e7"
      },
      "source": [
        "alpha = M / N\n",
        "print(alpha)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRPTX3DU5cUX",
        "outputId": "23ae770a-8813-4a0a-a029-50389ba7c6e8"
      },
      "source": [
        "D = torch.Tensor([[1,2,3],[2,3,4]])\n",
        "print(D)\n",
        "print(D.shape)\n",
        "print(\"M:\",len(D)) #M\n",
        "print(\"N:\",len(D[0])) #N"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [2., 3., 4.]])\n",
            "torch.Size([2, 3])\n",
            "M: 2\n",
            "N: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an1EjkYh5cWx",
        "outputId": "a57e0c21-5383-4938-83af-8b024155c7d1"
      },
      "source": [
        "def spin_overlap(x,y):\n",
        "  spin_overlap = 0\n",
        "  for i in range (len(x[0])) :\n",
        "\n",
        "    spin_overlap = spin_overlap + torch.dot(x.T[i],y.T[i])\n",
        "  return torch.abs(spin_overlap/(len(x)*len(x[0])))\n",
        "\n",
        "print(spin_overlap(S0,S0))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trrvsq5u5cY7"
      },
      "source": [
        "def bond_overlap(x,y):\n",
        "  bond_overlap = 0\n",
        "  for i in range (len(x[0])) :\n",
        "\n",
        "    bond_overlap = bond_overlap + torch.dot(x.T[i],y.T[i])\n",
        "  return torch.abs(bond_overlap/(len(x)*len(x[0])))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPFQ2zPq5cdG",
        "outputId": "bbffa47e-4b55-4c16-b89e-892b3c827768"
      },
      "source": [
        "S_a = torch.Tensor([[1,-1],[-1,1],[1,-1]])\n",
        "S_b = torch.Tensor([[1,-1],[1,1],[1,1]])\n",
        "print(spin_overlap(S_a,S_b))\n",
        "print(spin_overlap(S_a,S_a))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3333)\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiwcuRuk5cfM",
        "outputId": "02cfd135-4320-43ed-a2e1-9ae00227fccb"
      },
      "source": [
        "a1 = torch.dot(S_a.T[0],S_b.T[0])\n",
        "a2 = torch.dot(S_a.T[1],S_b.T[1])\n",
        "a3 = (a1 + a2)/(2*3)\n",
        "print(a3)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3333)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92eXGyfY5ciN"
      },
      "source": [
        "def MSE(t, y):\n",
        "    mse = torch.mean(torch.sum(torch.square(t-y),axis =1),axis = 0)\n",
        "    return mse"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nAwqcM08en9",
        "outputId": "fb3ac956-fbc3-4f8a-d763-20b0dd76dc9f"
      },
      "source": [
        "print(MSE(S0,SL))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(19.5556)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X76lQWy8eqf",
        "outputId": "7f667756-3739-4081-f3e0-4010dbd14c5d"
      },
      "source": [
        "A = torch.Tensor([[3,5,1],[4,12,1]])\n",
        "print(A**2)\n",
        "A_norm = (torch.sum(torch.square(A),axis=0))**(1/2)\n",
        "\n",
        "A_normalized = A / A_norm\n",
        "A_normalized_2 = A / A_norm * (N**(1/2))\n",
        "\n",
        "print(A_norm)\n",
        "\n",
        "print(A_normalized)\n",
        "print(A_normalized_2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  9.,  25.,   1.],\n",
            "        [ 16., 144.,   1.]])\n",
            "tensor([ 5.0000, 13.0000,  1.4142])\n",
            "tensor([[0.6000, 0.3846, 0.7071],\n",
            "        [0.8000, 0.9231, 0.7071]])\n",
            "tensor([[1.8974, 1.2163, 2.2361],\n",
            "        [2.5298, 2.9190, 2.2361]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I1vyyoy8eto"
      },
      "source": [
        "def weight_norm(x):\n",
        "    x_norm = (torch.sum(torch.square(x),axis=0)**(1/2))\n",
        "    return x_norm"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E53yYyVi8ewb",
        "outputId": "a86a80a5-3fc7-49fc-8389-771cacce3387"
      },
      "source": [
        "w_a = torch.Tensor([[1,0,1],[1,0,0],[0,1,0]]) \n",
        "w_b = torch.Tensor([[1,0,0],[0,1,0],[0,0,1]]) \n",
        "J_a = w_a / (weight_norm(w_a)) * (3**(1/2))\n",
        "J_b = w_b / (weight_norm(w_b)) * (3**(1/2))\n",
        "\n",
        "\n",
        "print(J_a)\n",
        "print(J_b)\n",
        "print(len(J_a[0]))\n",
        "\n",
        "print(bond_overlap(J_a,J_b))\n",
        "print(bond_overlap(J_a,J_a))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.2247, 0.0000, 1.7321],\n",
            "        [1.2247, 0.0000, 0.0000],\n",
            "        [0.0000, 1.7321, 0.0000]])\n",
            "tensor([[1.7321, 0.0000, 0.0000],\n",
            "        [0.0000, 1.7321, 0.0000],\n",
            "        [0.0000, 0.0000, 1.7321]])\n",
            "3\n",
            "tensor(0.2357)\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOiYrlSl8ey4"
      },
      "source": [
        "class Network1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network1, self).__init__()\n",
        "        self.fc1_1 = torch.nn.Linear(N, N, False)\n",
        "        self.fc2_1 = torch.nn.Linear(N, N, False)\n",
        "        self.fc3_1 = torch.nn.Linear(N, N, False)\n",
        "        self.fc4_1 = torch.nn.Linear(N, N, False)\n",
        "        self.fc5_1 = torch.nn.Linear(N, N, False)\n",
        "\n",
        "        torch.nn.init.normal_(self.fc1_1.weight, 0.0, 1.0)\n",
        "        torch.nn.init.normal_(self.fc2_1.weight, 0.0, 1.0)\n",
        "        torch.nn.init.normal_(self.fc3_1.weight, 0.0, 1.0)\n",
        "        torch.nn.init.normal_(self.fc4_1.weight, 0.0, 1.0)\n",
        "        torch.nn.init.normal_(self.fc5_1.weight, 0.0, 1.0)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.fc1_1(x))\n",
        "        S1_1 = torch.sign(x)\n",
        "        x = torch.tanh(self.fc2_1(x))\n",
        "        S2_1 = torch.sign(x)\n",
        "        x = torch.tanh(self.fc3_1(x))\n",
        "        S3_1 = torch.sign(x)\n",
        "        x = torch.tanh(self.fc4_1(x))\n",
        "        S4_1 = torch.sign(x)\n",
        "        x = torch.tanh(self.fc5_1(x))\n",
        "        S5_1 = torch.sign(x)\n",
        "\n",
        "        return S1_1, S2_1, S3_1, S4_1, S5_1\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj3EX9i78e1t",
        "outputId": "60d9f762-ac6d-4be6-936c-e77b3c540ac4"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # 学習回数\n",
        "    n_epoch = 1000\n",
        " \n",
        "    # 学習結果の保存用\n",
        "    history = {\n",
        "        'train_loss_1': [],\n",
        "    }\n",
        " \n",
        "    # ネットワークを構築\n",
        "    net: torch.nn.Module = Network1()\n",
        " \n",
        "    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.01)\n",
        " \n",
        "    for n in range(n_epoch):\n",
        " \n",
        "        \"\"\" Training Part\"\"\"\n",
        "        loss = None\n",
        "        # 学習開始 (再開)\n",
        "        net.train(True)  # 引数は省略可能\n",
        " \n",
        "        optimizer.zero_grad()\n",
        "        output = net(S0)\n",
        "        loss = MSE(output[0], SL)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "        print('EPOCH ', n + 1, ' | train_loss_1 ',loss)\n",
        "        \n",
        " \n",
        "        history['train_loss_1'].append(loss)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH  1  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  2  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  3  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  4  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  5  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  6  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  7  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  8  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  9  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  10  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  11  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  12  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  13  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  14  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  15  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  16  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  17  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  18  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  19  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  20  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  21  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  22  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  23  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  24  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  25  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  26  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  27  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  28  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  29  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  30  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  31  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  32  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  33  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  34  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  35  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  36  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  37  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  38  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  39  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  40  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  41  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  42  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  43  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  44  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  45  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  46  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  47  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  48  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  49  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  50  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  51  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  52  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  53  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  54  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  55  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  56  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  57  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  58  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  59  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  60  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  61  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  62  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  63  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  64  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  65  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  66  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  67  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  68  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  69  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  70  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  71  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  72  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  73  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  74  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  75  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  76  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  77  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  78  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  79  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  80  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  81  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  82  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  83  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  84  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  85  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  86  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  87  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  88  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  89  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  90  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  91  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  92  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  93  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  94  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  95  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  96  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  97  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  98  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  99  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  100  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  101  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  102  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  103  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  104  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  105  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  106  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  107  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  108  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  109  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  110  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  111  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  112  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  113  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  114  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  115  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  116  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  117  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  118  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  119  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  120  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  121  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  122  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  123  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  124  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  125  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  126  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  127  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  128  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  129  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  130  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  131  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  132  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  133  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  134  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  135  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  136  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  137  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  138  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  139  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  140  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  141  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  142  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  143  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  144  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  145  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  146  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  147  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  148  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  149  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  150  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  151  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  152  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  153  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  154  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  155  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  156  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  157  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  158  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  159  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  160  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  161  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  162  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  163  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  164  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  165  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  166  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  167  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  168  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  169  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  170  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  171  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  172  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  173  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  174  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  175  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  176  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  177  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  178  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  179  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  180  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  181  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  182  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  183  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  184  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  185  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  186  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  187  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  188  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  189  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  190  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  191  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  192  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  193  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  194  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  195  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  196  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  197  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  198  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  199  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  200  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  201  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  202  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  203  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  204  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  205  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  206  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  207  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  208  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  209  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  210  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  211  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  212  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  213  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  214  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  215  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  216  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  217  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  218  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  219  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  220  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  221  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  222  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  223  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  224  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  225  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  226  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  227  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  228  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  229  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  230  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  231  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  232  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  233  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  234  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  235  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  236  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  237  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  238  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  239  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  240  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  241  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  242  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  243  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  244  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  245  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  246  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  247  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  248  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  249  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  250  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  251  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  252  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  253  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  254  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  255  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  256  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  257  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  258  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  259  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  260  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  261  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  262  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  263  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  264  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  265  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  266  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  267  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  268  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  269  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  270  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  271  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  272  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  273  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  274  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  275  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  276  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  277  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  278  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  279  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  280  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  281  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  282  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  283  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  284  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  285  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  286  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  287  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  288  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  289  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  290  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  291  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  292  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  293  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  294  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  295  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  296  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  297  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  298  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  299  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  300  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  301  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  302  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  303  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  304  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  305  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  306  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  307  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  308  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  309  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  310  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  311  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  312  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  313  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  314  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  315  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  316  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  317  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  318  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  319  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  320  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  321  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  322  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  323  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  324  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  325  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  326  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  327  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  328  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  329  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  330  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  331  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  332  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  333  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  334  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  335  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  336  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  337  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  338  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  339  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  340  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  341  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  342  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  343  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  344  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  345  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  346  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  347  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  348  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  349  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  350  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  351  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  352  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  353  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  354  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  355  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  356  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  357  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  358  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  359  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  360  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  361  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  362  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  363  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  364  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  365  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  366  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  367  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  368  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  369  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  370  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  371  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  372  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  373  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  374  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  375  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  376  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  377  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  378  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  379  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  380  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  381  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  382  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  383  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  384  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  385  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  386  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  387  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  388  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  389  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  390  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  391  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  392  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  393  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  394  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  395  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  396  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  397  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  398  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  399  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  400  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  401  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  402  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  403  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  404  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  405  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  406  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  407  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  408  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  409  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  410  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  411  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  412  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  413  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  414  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  415  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  416  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  417  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  418  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  419  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  420  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  421  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  422  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  423  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  424  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  425  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  426  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  427  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  428  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  429  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  430  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  431  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  432  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  433  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  434  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  435  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  436  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  437  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  438  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  439  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  440  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  441  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  442  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  443  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  444  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  445  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  446  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  447  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  448  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  449  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  450  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  451  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  452  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  453  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  454  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  455  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  456  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  457  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  458  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  459  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  460  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  461  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  462  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  463  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  464  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  465  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  466  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  467  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  468  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  469  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  470  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  471  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  472  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  473  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  474  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  475  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  476  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  477  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  478  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  479  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  480  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  481  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  482  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  483  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  484  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  485  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  486  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  487  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  488  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  489  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  490  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  491  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  492  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  493  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  494  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  495  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  496  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  497  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  498  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  499  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  500  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  501  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  502  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  503  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  504  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  505  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  506  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  507  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  508  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  509  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  510  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  511  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  512  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  513  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  514  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  515  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  516  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  517  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  518  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  519  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  520  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  521  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  522  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  523  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  524  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  525  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  526  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  527  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  528  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  529  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  530  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  531  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  532  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  533  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  534  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  535  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  536  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  537  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  538  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  539  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  540  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  541  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  542  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  543  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  544  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  545  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  546  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  547  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  548  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  549  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  550  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  551  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  552  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  553  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  554  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  555  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  556  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  557  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  558  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  559  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  560  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  561  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  562  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  563  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  564  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  565  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  566  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  567  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  568  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  569  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  570  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  571  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  572  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  573  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  574  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  575  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  576  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  577  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  578  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  579  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  580  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  581  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  582  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  583  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  584  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  585  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  586  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  587  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  588  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  589  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  590  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  591  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  592  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  593  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  594  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  595  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  596  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  597  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  598  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  599  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  600  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  601  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  602  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  603  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  604  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  605  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  606  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  607  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  608  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  609  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  610  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  611  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  612  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  613  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  614  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  615  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  616  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  617  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  618  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  619  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  620  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  621  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  622  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  623  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  624  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  625  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  626  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  627  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  628  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  629  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  630  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  631  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  632  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  633  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  634  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  635  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  636  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  637  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  638  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  639  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  640  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  641  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  642  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  643  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  644  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  645  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  646  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  647  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  648  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  649  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  650  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  651  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  652  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  653  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  654  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  655  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  656  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  657  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  658  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  659  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  660  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  661  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  662  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  663  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  664  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  665  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  666  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  667  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  668  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  669  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  670  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  671  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  672  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  673  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  674  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  675  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  676  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  677  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  678  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  679  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  680  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  681  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  682  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  683  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  684  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  685  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  686  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  687  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  688  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  689  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  690  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  691  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  692  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  693  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  694  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  695  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  696  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  697  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  698  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  699  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  700  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  701  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  702  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  703  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  704  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  705  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  706  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  707  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  708  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  709  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  710  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  711  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  712  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  713  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  714  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  715  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  716  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  717  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  718  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  719  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  720  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  721  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  722  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  723  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  724  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  725  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  726  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  727  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  728  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  729  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  730  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  731  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  732  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  733  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  734  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  735  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  736  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  737  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  738  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  739  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  740  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  741  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  742  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  743  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  744  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  745  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  746  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  747  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  748  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  749  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  750  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  751  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  752  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  753  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  754  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  755  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  756  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  757  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  758  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  759  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  760  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  761  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  762  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  763  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  764  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  765  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  766  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  767  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  768  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  769  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  770  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  771  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  772  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  773  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  774  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  775  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  776  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  777  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  778  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  779  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  780  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  781  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  782  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  783  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  784  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  785  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  786  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  787  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  788  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  789  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  790  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  791  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  792  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  793  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  794  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  795  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  796  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  797  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  798  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  799  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  800  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  801  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  802  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  803  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  804  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  805  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  806  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  807  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  808  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  809  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  810  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  811  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  812  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  813  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  814  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  815  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  816  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  817  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  818  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  819  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  820  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  821  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  822  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  823  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  824  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  825  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  826  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  827  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  828  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  829  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  830  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  831  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  832  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  833  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  834  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  835  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  836  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  837  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  838  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  839  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  840  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  841  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  842  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  843  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  844  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  845  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  846  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  847  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  848  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  849  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  850  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  851  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  852  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  853  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  854  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  855  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  856  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  857  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  858  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  859  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  860  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  861  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  862  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  863  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  864  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  865  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  866  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  867  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  868  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  869  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  870  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  871  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  872  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  873  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  874  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  875  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  876  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  877  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  878  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  879  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  880  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  881  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  882  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  883  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  884  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  885  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  886  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  887  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  888  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  889  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  890  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  891  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  892  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  893  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  894  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  895  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  896  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  897  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  898  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  899  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  900  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  901  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  902  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  903  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  904  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  905  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  906  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  907  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  908  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  909  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  910  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  911  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  912  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  913  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  914  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  915  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  916  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  917  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  918  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  919  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  920  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  921  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  922  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  923  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  924  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  925  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  926  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  927  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  928  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  929  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  930  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  931  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  932  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  933  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  934  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  935  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  936  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  937  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  938  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  939  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  940  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  941  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  942  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  943  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  944  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  945  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  946  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  947  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  948  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  949  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  950  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  951  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  952  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  953  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  954  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  955  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  956  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  957  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  958  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  959  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  960  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  961  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  962  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  963  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  964  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  965  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  966  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  967  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  968  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  969  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  970  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  971  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  972  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  973  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  974  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  975  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  976  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  977  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  978  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  979  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  980  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  981  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  982  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  983  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  984  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  985  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  986  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  987  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  988  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  989  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  990  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  991  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  992  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  993  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  994  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  995  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  996  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  997  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  998  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  999  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n",
            "EPOCH  1000  | train_loss_1  tensor(22.2222, grad_fn=<MeanBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "UhQTBQbK8e6N",
        "outputId": "f3137fea-3ddc-4483-9b5b-71f42bf71011"
      },
      "source": [
        "# 結果の出力と描画\n",
        "print(history)\n",
        "plt.figure()\n",
        "plt.plot(range(1, n_epoch+1), history['train_loss_1'], label='train_loss_1')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png')\n",
        " \n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train_loss_1': [tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>), tensor(22.2222, grad_fn=<MeanBackward1>)]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35U3BMAWCORT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}